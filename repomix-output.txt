This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: raw-data/, ai-docs/, uv.lock
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.claude/
  settings.local.json
src/
  data_ingestion/
    __init__.py
    ingest_accounts.py
    ingest_metrics.py
    ingest_plans.py
    ingest_regimes.py
    ingest_trades.py
  db_schema/
    schema.sql
  feature_engineering/
    __init__.py
    build_training_data.py
    engineer_features.py
  modeling/
    __init__.py
    predict_daily.py
    train_model.py
  pipeline_orchestration/
    __init__.py
    run_pipeline.py
  preprocessing/
    __init__.py
    create_staging_snapshots.py
  utils/
    __init__.py
    api_client.py
    database.py
    logging_config.py
.gitignore
.python-version
pyproject.toml
README.md

================================================================
Files
================================================================

================
File: .claude/settings.local.json
================
{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)"
    ],
    "deny": []
  }
}

================
File: src/data_ingestion/__init__.py
================
# Data ingestion modules for the daily profit model

================
File: src/data_ingestion/ingest_accounts.py
================
"""
Ingest accounts data from the /accounts API endpoint.
Stores data in the raw_accounts_data table.
"""

import os
import sys
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
import argparse

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.api_client import RiskAnalyticsAPIClient
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class AccountsIngester:
    """Handles ingestion of accounts data from the API."""
    
    def __init__(self):
        """Initialize the accounts ingester."""
        self.db_manager = get_db_manager()
        self.api_client = RiskAnalyticsAPIClient()
        self.table_name = 'raw_accounts_data'
        self.source_endpoint = '/accounts'
        
    def ingest_accounts(self, 
                       logins: Optional[List[str]] = None,
                       traders: Optional[List[str]] = None,
                       force_full_refresh: bool = False) -> int:
        """
        Ingest accounts data from the API.
        
        Args:
            logins: Optional list of specific login IDs to fetch
            traders: Optional list of specific trader IDs to fetch
            force_full_refresh: If True, truncate existing data and reload all
            
        Returns:
            Number of records ingested
        """
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_accounts',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Check if we need to do a full refresh
            if force_full_refresh:
                logger.warning("Force full refresh requested. Truncating existing data.")
                self.db_manager.model_db.execute_command(f"TRUNCATE TABLE {self.table_name}")
            
            # Prepare batch for insertion
            batch_data = []
            batch_size = 1000
            
            # Fetch accounts data with pagination
            logger.info("Starting accounts data ingestion...")
            for page_num, accounts_page in enumerate(self.api_client.get_accounts(
                logins=logins, 
                traders=traders
            )):
                logger.info(f"Processing page {page_num + 1} with {len(accounts_page)} accounts")
                
                for account in accounts_page:
                    # Transform account data for database insertion
                    record = self._transform_account_record(account)
                    batch_data.append(record)
                    
                    # Insert batch when it reaches the size limit
                    if len(batch_data) >= batch_size:
                        self._insert_batch(batch_data)
                        total_records += len(batch_data)
                        batch_data = []
                
            # Insert any remaining records
            if batch_data:
                self._insert_batch(batch_data)
                total_records += len(batch_data)
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_accounts',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'force_full_refresh': force_full_refresh
                }
            )
            
            logger.info(f"Successfully ingested {total_records} account records")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_accounts',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to ingest accounts data: {str(e)}")
            raise
    
    def _transform_account_record(self, account: Dict[str, Any]) -> Dict[str, Any]:
        """
        Transform API account record to database format.
        
        Args:
            account: Raw account data from API
            
        Returns:
            Transformed record ready for database insertion
        """
        return {
            'account_id': account.get('accountId'),
            'login': account.get('login'),
            'trader_id': account.get('traderId'),
            'plan_id': account.get('planId'),
            'starting_balance': account.get('startingBalance'),
            'current_balance': account.get('currentBalance'),
            'current_equity': account.get('currentEquity'),
            'profit_target_pct': account.get('profitTargetPct'),
            'max_daily_drawdown_pct': account.get('maxDailyDrawdownPct'),
            'max_drawdown_pct': account.get('maxDrawdownPct'),
            'max_leverage': account.get('maxLeverage'),
            'is_drawdown_relative': account.get('isDrawdownRelative'),
            'breached': account.get('breached', 0),
            'is_upgraded': account.get('isUpgraded', 0),
            'phase': account.get('phase'),
            'status': account.get('status'),
            'created_at': account.get('createdAt'),
            'updated_at': account.get('updatedAt'),
            'ingestion_timestamp': datetime.now(),
            'source_api_endpoint': self.source_endpoint
        }
    
    def _insert_batch(self, batch_data: List[Dict[str, Any]]):
        """Insert a batch of records into the database."""
        try:
            # Use ON CONFLICT to handle duplicates
            query = f"""
            INSERT INTO {self.table_name} 
            ({', '.join(batch_data[0].keys())})
            VALUES ({', '.join(['%s'] * len(batch_data[0]))})
            ON CONFLICT (account_id, ingestion_timestamp) DO NOTHING
            """
            
            # Convert to list of tuples for bulk insert
            values = [tuple(record.values()) for record in batch_data]
            
            with self.db_manager.model_db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.executemany(query, values)
            
            logger.debug(f"Inserted batch of {len(batch_data)} records")
            
        except Exception as e:
            logger.error(f"Failed to insert batch: {str(e)}")
            raise
    
    def close(self):
        """Clean up resources."""
        self.api_client.close()


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Ingest accounts data from Risk Analytics API')
    parser.add_argument('--logins', nargs='+', help='Specific login IDs to fetch')
    parser.add_argument('--traders', nargs='+', help='Specific trader IDs to fetch')
    parser.add_argument('--force-refresh', action='store_true', 
                       help='Force full refresh (truncate and reload)')
    parser.add_argument('--log-level', default='INFO', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='ingest_accounts')
    
    # Run ingestion
    ingester = AccountsIngester()
    try:
        records = ingester.ingest_accounts(
            logins=args.logins,
            traders=args.traders,
            force_full_refresh=args.force_refresh
        )
        logger.info(f"Ingestion complete. Total records: {records}")
    finally:
        ingester.close()


if __name__ == '__main__':
    main()

================
File: src/data_ingestion/ingest_metrics.py
================
"""
Ingest metrics data from the /metrics API endpoints (alltime, daily, hourly).
Stores data in the respective raw_metrics_* tables.
"""

import os
import sys
import logging
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Optional
import argparse

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.api_client import RiskAnalyticsAPIClient
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class MetricsIngester:
    """Handles ingestion of metrics data from the API."""
    
    def __init__(self):
        """Initialize the metrics ingester."""
        self.db_manager = get_db_manager()
        self.api_client = RiskAnalyticsAPIClient()
        
        # Table mapping for different metric types
        self.table_mapping = {
            'alltime': 'raw_metrics_alltime',
            'daily': 'raw_metrics_daily',
            'hourly': 'raw_metrics_hourly'
        }
    
    def ingest_metrics(self,
                      metric_type: str,
                      start_date: Optional[date] = None,
                      end_date: Optional[date] = None,
                      logins: Optional[List[str]] = None,
                      accountids: Optional[List[str]] = None,
                      force_full_refresh: bool = False) -> int:
        """
        Ingest metrics data from the API.
        
        Args:
            metric_type: Type of metrics ('alltime', 'daily', 'hourly')
            start_date: Start date for daily/hourly metrics
            end_date: End date for daily/hourly metrics
            logins: Optional list of specific login IDs
            accountids: Optional list of specific account IDs
            force_full_refresh: If True, truncate existing data and reload
            
        Returns:
            Number of records ingested
        """
        if metric_type not in self.table_mapping:
            raise ValueError(f"Invalid metric type: {metric_type}")
        
        table_name = self.table_mapping[metric_type]
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage=f'ingest_metrics_{metric_type}',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Handle full refresh
            if force_full_refresh:
                logger.warning(f"Force full refresh requested for {metric_type}. Truncating existing data.")
                self.db_manager.model_db.execute_command(f"TRUNCATE TABLE {table_name}")
            
            # Ingest based on metric type
            if metric_type == 'alltime':
                total_records = self._ingest_alltime_metrics(
                    table_name, logins, accountids
                )
            else:
                total_records = self._ingest_time_series_metrics(
                    metric_type, table_name, start_date, end_date, 
                    logins, accountids
                )
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage=f'ingest_metrics_{metric_type}',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'start_date': str(start_date) if start_date else None,
                    'end_date': str(end_date) if end_date else None,
                    'force_full_refresh': force_full_refresh
                }
            )
            
            logger.info(f"Successfully ingested {total_records} {metric_type} metric records")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage=f'ingest_metrics_{metric_type}',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to ingest {metric_type} metrics: {str(e)}")
            raise
    
    def _ingest_alltime_metrics(self,
                               table_name: str,
                               logins: Optional[List[str]],
                               accountids: Optional[List[str]]) -> int:
        """Ingest all-time metrics data."""
        batch_data = []
        batch_size = 1000
        total_records = 0
        
        logger.info("Starting all-time metrics ingestion...")
        
        for page_num, metrics_page in enumerate(self.api_client.get_metrics(
            metric_type='alltime',
            logins=logins,
            accountids=accountids
        )):
            logger.info(f"Processing page {page_num + 1} with {len(metrics_page)} metrics")
            
            for metric in metrics_page:
                record = self._transform_alltime_metric(metric)
                batch_data.append(record)
                
                if len(batch_data) >= batch_size:
                    self._insert_metrics_batch(batch_data, table_name)
                    total_records += len(batch_data)
                    batch_data = []
        
        # Insert remaining records
        if batch_data:
            self._insert_metrics_batch(batch_data, table_name)
            total_records += len(batch_data)
        
        return total_records
    
    def _ingest_time_series_metrics(self,
                                  metric_type: str,
                                  table_name: str,
                                  start_date: Optional[date],
                                  end_date: Optional[date],
                                  logins: Optional[List[str]],
                                  accountids: Optional[List[str]]) -> int:
        """Ingest daily or hourly metrics data."""
        # Determine date range
        if not end_date:
            end_date = datetime.now().date() - timedelta(days=1)  # Yesterday
        if not start_date:
            # Default to last 30 days for initial load
            start_date = end_date - timedelta(days=30)
        
        batch_data = []
        batch_size = 1000
        total_records = 0
        
        # Process date by date to manage volume (especially for hourly)
        current_date = start_date
        while current_date <= end_date:
            date_str = self.api_client.format_date(current_date)
            logger.info(f"Processing {metric_type} metrics for date: {date_str}")
            
            # For hourly metrics, we might want to specify hours
            hours = list(range(24)) if metric_type == 'hourly' else None
            
            for page_num, metrics_page in enumerate(self.api_client.get_metrics(
                metric_type=metric_type,
                logins=logins,
                accountids=accountids,
                dates=[date_str],
                hours=hours
            )):
                logger.debug(f"Date {date_str}, page {page_num + 1}: {len(metrics_page)} records")
                
                for metric in metrics_page:
                    if metric_type == 'daily':
                        record = self._transform_daily_metric(metric)
                    else:  # hourly
                        record = self._transform_hourly_metric(metric)
                    
                    batch_data.append(record)
                    
                    if len(batch_data) >= batch_size:
                        self._insert_metrics_batch(batch_data, table_name)
                        total_records += len(batch_data)
                        batch_data = []
            
            current_date += timedelta(days=1)
        
        # Insert remaining records
        if batch_data:
            self._insert_metrics_batch(batch_data, table_name)
            total_records += len(batch_data)
        
        return total_records
    
    def _transform_alltime_metric(self, metric: Dict[str, Any]) -> Dict[str, Any]:
        """Transform all-time metric record for database."""
        return {
            'account_id': metric.get('accountId'),
            'login': metric.get('login'),
            'net_profit': metric.get('netProfit'),
            'gross_profit': metric.get('grossProfit'),
            'gross_loss': metric.get('grossLoss'),
            'total_trades': metric.get('totalTrades'),
            'winning_trades': metric.get('winningTrades'),
            'losing_trades': metric.get('losingTrades'),
            'win_rate': metric.get('winRate'),
            'profit_factor': metric.get('profitFactor'),
            'average_win': metric.get('averageWin'),
            'average_loss': metric.get('averageLoss'),
            'average_rrr': metric.get('averageRRR'),
            'expectancy': metric.get('expectancy'),
            'sharpe_ratio': metric.get('sharpeRatio'),
            'sortino_ratio': metric.get('sortinoRatio'),
            'max_drawdown': metric.get('maxDrawdown'),
            'max_drawdown_pct': metric.get('maxDrawdownPct'),
            'ingestion_timestamp': datetime.now(),
            'source_api_endpoint': '/v2/metrics/alltime'
        }
    
    def _transform_daily_metric(self, metric: Dict[str, Any]) -> Dict[str, Any]:
        """Transform daily metric record for database."""
        # Parse date from YYYYMMDD format
        date_str = str(metric.get('date', ''))
        if date_str and len(date_str) == 8:
            metric_date = datetime.strptime(date_str, '%Y%m%d').date()
        else:
            metric_date = None
        
        return {
            'account_id': metric.get('accountId'),
            'login': metric.get('login'),
            'date': metric_date,
            'net_profit': metric.get('netProfit'),  # TARGET VARIABLE
            'gross_profit': metric.get('grossProfit'),
            'gross_loss': metric.get('grossLoss'),
            'total_trades': metric.get('totalTrades'),
            'winning_trades': metric.get('winningTrades'),
            'losing_trades': metric.get('losingTrades'),
            'win_rate': metric.get('winRate'),
            'profit_factor': metric.get('profitFactor'),
            'lots_traded': metric.get('lotsTraded'),
            'volume_traded': metric.get('volumeTraded'),
            'commission': metric.get('commission'),
            'swap': metric.get('swap'),
            'balance_start': metric.get('balanceStart'),
            'balance_end': metric.get('balanceEnd'),
            'equity_start': metric.get('equityStart'),
            'equity_end': metric.get('equityEnd'),
            'ingestion_timestamp': datetime.now(),
            'source_api_endpoint': '/v2/metrics/daily'
        }
    
    def _transform_hourly_metric(self, metric: Dict[str, Any]) -> Dict[str, Any]:
        """Transform hourly metric record for database."""
        # Parse date from YYYYMMDD format
        date_str = str(metric.get('date', ''))
        if date_str and len(date_str) == 8:
            metric_date = datetime.strptime(date_str, '%Y%m%d').date()
        else:
            metric_date = None
        
        return {
            'account_id': metric.get('accountId'),
            'login': metric.get('login'),
            'date': metric_date,
            'hour': metric.get('hour'),
            'net_profit': metric.get('netProfit'),
            'gross_profit': metric.get('grossProfit'),
            'gross_loss': metric.get('grossLoss'),
            'total_trades': metric.get('totalTrades'),
            'winning_trades': metric.get('winningTrades'),
            'losing_trades': metric.get('losingTrades'),
            'win_rate': metric.get('winRate'),
            'lots_traded': metric.get('lotsTraded'),
            'volume_traded': metric.get('volumeTraded'),
            'ingestion_timestamp': datetime.now(),
            'source_api_endpoint': '/v2/metrics/hourly'
        }
    
    def _insert_metrics_batch(self, batch_data: List[Dict[str, Any]], table_name: str):
        """Insert a batch of metric records into the database."""
        if not batch_data:
            return
        
        try:
            # Build the insert query with ON CONFLICT
            columns = list(batch_data[0].keys())
            placeholders = ', '.join(['%s'] * len(columns))
            columns_str = ', '.join(columns)
            
            # Determine conflict columns based on table
            if 'alltime' in table_name:
                conflict_cols = 'account_id, ingestion_timestamp'
            elif 'daily' in table_name:
                conflict_cols = 'account_id, date, ingestion_timestamp'
            else:  # hourly
                conflict_cols = 'account_id, date, hour, ingestion_timestamp'
            
            query = f"""
            INSERT INTO {table_name} ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT ({conflict_cols}) DO NOTHING
            """
            
            # Convert to list of tuples
            values = [tuple(record[col] for col in columns) for record in batch_data]
            
            with self.db_manager.model_db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.executemany(query, values)
            
            logger.debug(f"Inserted batch of {len(batch_data)} records into {table_name}")
            
        except Exception as e:
            logger.error(f"Failed to insert batch into {table_name}: {str(e)}")
            raise
    
    def close(self):
        """Clean up resources."""
        self.api_client.close()


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Ingest metrics data from Risk Analytics API')
    parser.add_argument('metric_type', choices=['alltime', 'daily', 'hourly'],
                       help='Type of metrics to ingest')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Start date for daily/hourly metrics (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='End date for daily/hourly metrics (YYYY-MM-DD)')
    parser.add_argument('--logins', nargs='+', help='Specific login IDs to fetch')
    parser.add_argument('--accountids', nargs='+', help='Specific account IDs to fetch')
    parser.add_argument('--force-refresh', action='store_true',
                       help='Force full refresh (truncate and reload)')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file=f'ingest_metrics_{args.metric_type}')
    
    # Run ingestion
    ingester = MetricsIngester()
    try:
        records = ingester.ingest_metrics(
            metric_type=args.metric_type,
            start_date=args.start_date,
            end_date=args.end_date,
            logins=args.logins,
            accountids=args.accountids,
            force_full_refresh=args.force_refresh
        )
        logger.info(f"Ingestion complete. Total records: {records}")
    finally:
        ingester.close()


if __name__ == '__main__':
    main()

================
File: src/data_ingestion/ingest_plans.py
================
"""
Ingest plans data from CSV files.
Stores data in the raw_plans_data table.
"""

import os
import sys
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import argparse
import pandas as pd
import glob

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class PlansIngester:
    """Handles ingestion of plans data from CSV files."""
    
    def __init__(self):
        """Initialize the plans ingester."""
        self.db_manager = get_db_manager()
        self.table_name = 'raw_plans_data'
        
        # Expected column mappings from CSV to database
        self.column_mapping = {
            'Plan ID': 'plan_id',
            'PlanID': 'plan_id',
            'plan_id': 'plan_id',
            'Plan Name': 'plan_name',
            'PlanName': 'plan_name',
            'plan_name': 'plan_name',
            'Type': 'plan_type',
            'type': 'plan_type',
            'Starting Balance': 'starting_balance',
            'starting_balance': 'starting_balance',
            'Profit Target': 'profit_target',
            'profit_target': 'profit_target',
            'Profit Target %': 'profit_target_pct',
            'profit_target_pct': 'profit_target_pct',
            'Max Drawdown': 'max_drawdown',
            'max_drawdown': 'max_drawdown',
            'Max Drawdown %': 'max_drawdown_pct',
            'max_drawdown_pct': 'max_drawdown_pct',
            'Max Daily Drawdown': 'max_daily_drawdown',
            'max_daily_drawdown': 'max_daily_drawdown',
            'Max Daily Drawdown %': 'max_daily_drawdown_pct',
            'max_daily_drawdown_pct': 'max_daily_drawdown_pct',
            'Max Leverage': 'max_leverage',
            'max_leverage': 'max_leverage',
            'Is Drawdown Relative': 'is_drawdown_relative',
            'is_drawdown_relative': 'is_drawdown_relative',
            'Min Trading Days': 'min_trading_days',
            'min_trading_days': 'min_trading_days',
            'Max Trading Days': 'max_trading_days',
            'max_trading_days': 'max_trading_days',
            'Profit Split %': 'profit_split_pct',
            'profit_split_pct': 'profit_split_pct'
        }
    
    def ingest_plans(self, 
                    csv_directory: str = None,
                    specific_files: Optional[List[str]] = None,
                    force_full_refresh: bool = False) -> int:
        """
        Ingest plans data from CSV files.
        
        Args:
            csv_directory: Directory containing CSV files (defaults to raw-data/plans)
            specific_files: List of specific CSV files to process
            force_full_refresh: If True, truncate existing data and reload all
            
        Returns:
            Number of records ingested
        """
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_plans',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Handle full refresh
            if force_full_refresh:
                logger.warning("Force full refresh requested for plans. Truncating existing data.")
                self.db_manager.model_db.execute_command(f"TRUNCATE TABLE {self.table_name}")
            
            # Determine CSV files to process
            if specific_files:
                csv_files = specific_files
            else:
                if not csv_directory:
                    # Default to raw-data/plans relative to project root
                    project_root = Path(__file__).parent.parent.parent
                    csv_directory = project_root / "raw-data" / "plans"
                else:
                    csv_directory = Path(csv_directory)
                
                # Find all CSV files in the directory
                csv_files = list(csv_directory.glob("*.csv"))
                logger.info(f"Found {len(csv_files)} CSV files in {csv_directory}")
            
            # Process each CSV file
            for csv_file in csv_files:
                file_records = self._process_csv_file(csv_file)
                total_records += file_records
                logger.info(f"Processed {file_records} records from {csv_file}")
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_plans',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'files_processed': len(csv_files),
                    'force_full_refresh': force_full_refresh
                }
            )
            
            logger.info(f"Successfully ingested {total_records} plan records from {len(csv_files)} files")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_plans',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to ingest plans data: {str(e)}")
            raise
    
    def _process_csv_file(self, csv_file: Path) -> int:
        """
        Process a single CSV file and insert records into the database.
        
        Args:
            csv_file: Path to the CSV file
            
        Returns:
            Number of records processed from this file
        """
        try:
            logger.info(f"Processing CSV file: {csv_file}")
            
            # Read CSV file
            df = pd.read_csv(csv_file)
            logger.info(f"Read {len(df)} rows from {csv_file}")
            
            # Rename columns based on mapping
            df_renamed = df.rename(columns=self.column_mapping)
            
            # Add ingestion timestamp
            df_renamed['ingestion_timestamp'] = datetime.now()
            
            # Convert DataFrame to list of dictionaries
            records = df_renamed.to_dict('records')
            
            # Clean and transform records
            cleaned_records = []
            for record in records:
                cleaned_record = self._transform_plan_record(record)
                if cleaned_record:
                    cleaned_records.append(cleaned_record)
            
            # Insert records in batches
            batch_size = 100
            total_inserted = 0
            
            for i in range(0, len(cleaned_records), batch_size):
                batch = cleaned_records[i:i + batch_size]
                self._insert_plans_batch(batch)
                total_inserted += len(batch)
            
            return total_inserted
            
        except Exception as e:
            logger.error(f"Failed to process CSV file {csv_file}: {str(e)}")
            raise
    
    def _transform_plan_record(self, record: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Transform and clean a plan record from CSV.
        
        Args:
            record: Raw record from CSV
            
        Returns:
            Cleaned record ready for database insertion, or None if invalid
        """
        # Extract only the columns we need
        db_columns = [
            'plan_id', 'plan_name', 'plan_type', 'starting_balance',
            'profit_target', 'profit_target_pct', 'max_drawdown', 'max_drawdown_pct',
            'max_daily_drawdown', 'max_daily_drawdown_pct', 'max_leverage',
            'is_drawdown_relative', 'min_trading_days', 'max_trading_days',
            'profit_split_pct', 'ingestion_timestamp'
        ]
        
        cleaned_record = {}
        for col in db_columns:
            if col in record:
                value = record[col]
                
                # Handle NaN values
                if pd.isna(value):
                    value = None
                
                # Convert boolean strings to actual booleans
                if col == 'is_drawdown_relative' and value is not None:
                    if isinstance(value, str):
                        value = value.lower() in ['true', 'yes', '1', 't', 'y']
                    else:
                        value = bool(value)
                
                # Convert percentage strings to decimals
                if col.endswith('_pct') and value is not None:
                    if isinstance(value, str) and value.endswith('%'):
                        value = float(value.rstrip('%'))
                
                cleaned_record[col] = value
            else:
                # Set default values for missing columns
                if col == 'ingestion_timestamp':
                    cleaned_record[col] = datetime.now()
                else:
                    cleaned_record[col] = None
        
        # Validate required fields
        if not cleaned_record.get('plan_id'):
            logger.warning(f"Skipping record with missing plan_id: {record}")
            return None
        
        return cleaned_record
    
    def _insert_plans_batch(self, batch_data: List[Dict[str, Any]]):
        """Insert a batch of plan records into the database."""
        if not batch_data:
            return
        
        try:
            # Build the insert query with ON CONFLICT
            columns = list(batch_data[0].keys())
            placeholders = ', '.join(['%s'] * len(columns))
            columns_str = ', '.join(columns)
            
            query = f"""
            INSERT INTO {self.table_name} ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT (plan_id, ingestion_timestamp) DO NOTHING
            """
            
            # Convert to list of tuples
            values = [tuple(record[col] for col in columns) for record in batch_data]
            
            with self.db_manager.model_db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.executemany(query, values)
            
            logger.debug(f"Inserted batch of {len(batch_data)} plan records")
            
        except Exception as e:
            logger.error(f"Failed to insert batch: {str(e)}")
            if batch_data:
                logger.error(f"Sample record: {batch_data[0]}")
            raise


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Ingest plans data from CSV files')
    parser.add_argument('--csv-dir', help='Directory containing CSV files')
    parser.add_argument('--files', nargs='+', help='Specific CSV files to process')
    parser.add_argument('--force-refresh', action='store_true',
                       help='Force full refresh (truncate and reload)')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='ingest_plans')
    
    # Run ingestion
    ingester = PlansIngester()
    try:
        # Convert file paths to Path objects if provided
        specific_files = [Path(f) for f in args.files] if args.files else None
        
        records = ingester.ingest_plans(
            csv_directory=args.csv_dir,
            specific_files=specific_files,
            force_full_refresh=args.force_refresh
        )
        logger.info(f"Ingestion complete. Total records: {records}")
    except Exception as e:
        logger.error(f"Ingestion failed: {str(e)}")
        raise


if __name__ == '__main__':
    main()

================
File: src/data_ingestion/ingest_regimes.py
================
"""
Ingest regimes_daily data from the Supabase public.regimes_daily table.
Stores data in the raw_regimes_daily table.
"""

import os
import sys
import logging
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Optional
import argparse
import json
import numpy as np

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class RegimesIngester:
    """Handles ingestion of regimes_daily data from Supabase."""
    
    def __init__(self):
        """Initialize the regimes ingester."""
        self.db_manager = get_db_manager()
        self.table_name = 'raw_regimes_daily'
        
    def ingest_regimes(self,
                      start_date: Optional[date] = None,
                      end_date: Optional[date] = None,
                      force_full_refresh: bool = False) -> int:
        """
        Ingest regimes_daily data from the source Supabase table.
        
        Args:
            start_date: Start date for regime data
            end_date: End date for regime data
            force_full_refresh: If True, truncate existing data and reload all
            
        Returns:
            Number of records ingested
        """
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_regimes',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Handle full refresh
            if force_full_refresh:
                logger.warning("Force full refresh requested for regimes. Truncating existing data.")
                self.db_manager.model_db.execute_command(f"TRUNCATE TABLE {self.table_name}")
            
            # Determine date range
            if not end_date:
                end_date = datetime.now().date() - timedelta(days=1)  # Yesterday
            if not start_date:
                if force_full_refresh:
                    # For full refresh, go back further
                    start_date = end_date - timedelta(days=365)  # Last year
                else:
                    # For incremental, default to last 30 days
                    start_date = end_date - timedelta(days=30)
            
            # Query source data
            logger.info(f"Querying regimes_daily from {start_date} to {end_date}")
            
            query = """
            SELECT 
                date,
                market_news,
                instruments,
                country_economic_indicators,
                news_analysis,
                summary,
                vector_daily_regime,
                created_at,
                updated_at
            FROM public.regimes_daily
            WHERE date >= %s AND date <= %s
            ORDER BY date
            """
            
            # Fetch data from source database
            with self.db_manager.source_db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute(query, (start_date, end_date))
                    
                    # Process in batches to handle large datasets
                    batch_size = 100
                    batch_data = []
                    
                    while True:
                        rows = cursor.fetchmany(batch_size)
                        if not rows:
                            break
                        
                        # Transform rows
                        for row in rows:
                            record = self._transform_regime_record(row)
                            batch_data.append(record)
                        
                        # Insert batch when full
                        if len(batch_data) >= batch_size:
                            self._insert_regimes_batch(batch_data)
                            total_records += len(batch_data)
                            batch_data = []
                            
                            # Log progress
                            if total_records % 1000 == 0:
                                logger.info(f"Progress: {total_records} records processed")
                    
                    # Insert remaining records
                    if batch_data:
                        self._insert_regimes_batch(batch_data)
                        total_records += len(batch_data)
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_regimes',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'start_date': str(start_date),
                    'end_date': str(end_date),
                    'force_full_refresh': force_full_refresh
                }
            )
            
            logger.info(f"Successfully ingested {total_records} regime records")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='ingest_regimes',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to ingest regimes data: {str(e)}")
            raise
    
    def _transform_regime_record(self, row: tuple) -> Dict[str, Any]:
        """
        Transform a regime record from the source database.
        
        Args:
            row: Row from cursor fetchmany
            
        Returns:
            Transformed record ready for insertion
        """
        # Unpack the row based on our SELECT statement order
        (date_val, market_news, instruments, country_economic_indicators,
         news_analysis, summary, vector_daily_regime, created_at, updated_at) = row
        
        # Handle the vector_daily_regime conversion
        # It might come as a list, numpy array, or string representation
        if vector_daily_regime is not None:
            if isinstance(vector_daily_regime, str):
                # Parse string representation of array
                try:
                    vector_daily_regime = json.loads(vector_daily_regime)
                except:
                    # Try numpy-style parsing
                    vector_daily_regime = np.fromstring(
                        vector_daily_regime.strip('[]'), sep=','
                    ).tolist()
            elif isinstance(vector_daily_regime, np.ndarray):
                vector_daily_regime = vector_daily_regime.tolist()
            # Ensure it's a list of floats
            if isinstance(vector_daily_regime, list):
                vector_daily_regime = [float(x) for x in vector_daily_regime]
        
        return {
            'date': date_val,
            'market_news': json.dumps(market_news) if isinstance(market_news, dict) else market_news,
            'instruments': json.dumps(instruments) if isinstance(instruments, dict) else instruments,
            'country_economic_indicators': json.dumps(country_economic_indicators) if isinstance(country_economic_indicators, dict) else country_economic_indicators,
            'news_analysis': json.dumps(news_analysis) if isinstance(news_analysis, dict) else news_analysis,
            'summary': json.dumps(summary) if isinstance(summary, dict) else summary,
            'vector_daily_regime': vector_daily_regime,
            'created_at': created_at,
            'updated_at': updated_at,
            'ingestion_timestamp': datetime.now()
        }
    
    def _insert_regimes_batch(self, batch_data: List[Dict[str, Any]]):
        """Insert a batch of regime records into the database."""
        if not batch_data:
            return
        
        try:
            # Build the insert query with ON CONFLICT
            query = """
            INSERT INTO {} (
                date, market_news, instruments, country_economic_indicators,
                news_analysis, summary, vector_daily_regime, created_at,
                updated_at, ingestion_timestamp
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (date, ingestion_timestamp) DO NOTHING
            """.format(self.table_name)
            
            # Convert to list of tuples
            values = [
                (
                    record['date'],
                    record['market_news'],
                    record['instruments'],
                    record['country_economic_indicators'],
                    record['news_analysis'],
                    record['summary'],
                    record['vector_daily_regime'],
                    record['created_at'],
                    record['updated_at'],
                    record['ingestion_timestamp']
                )
                for record in batch_data
            ]
            
            with self.db_manager.model_db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.executemany(query, values)
            
            logger.debug(f"Inserted batch of {len(batch_data)} regime records")
            
        except Exception as e:
            logger.error(f"Failed to insert batch: {str(e)}")
            if batch_data:
                logger.error(f"Sample record date: {batch_data[0].get('date')}")
            raise
    
    def get_latest_ingested_date(self) -> Optional[date]:
        """Get the latest date already ingested to support incremental loads."""
        query = f"SELECT MAX(date) as max_date FROM {self.table_name}"
        
        try:
            result = self.db_manager.model_db.execute_query(query)
            if result and result[0]['max_date']:
                return result[0]['max_date']
        except Exception as e:
            logger.warning(f"Could not get latest date: {str(e)}")
        
        return None


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Ingest regimes_daily data from Supabase')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Start date for regimes data (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='End date for regimes data (YYYY-MM-DD)')
    parser.add_argument('--force-refresh', action='store_true',
                       help='Force full refresh (truncate and reload)')
    parser.add_argument('--incremental', action='store_true',
                       help='Perform incremental load from last ingested date')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='ingest_regimes')
    
    # Run ingestion
    ingester = RegimesIngester()
    try:
        # Handle incremental load
        start_date = args.start_date
        if args.incremental and not args.force_refresh:
            latest_date = ingester.get_latest_ingested_date()
            if latest_date:
                start_date = latest_date + timedelta(days=1)
                logger.info(f"Incremental load from {start_date}")
        
        records = ingester.ingest_regimes(
            start_date=start_date,
            end_date=args.end_date,
            force_full_refresh=args.force_refresh
        )
        logger.info(f"Ingestion complete. Total records: {records}")
    except Exception as e:
        logger.error(f"Ingestion failed: {str(e)}")
        raise


if __name__ == '__main__':
    main()

================
File: src/data_ingestion/ingest_trades.py
================
"""
Ingest trades data from the /trades API endpoints (closed and open).
Handles the large volume of closed trades (81M records) efficiently.
"""

import os
import sys
import logging
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Optional
import argparse

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.api_client import RiskAnalyticsAPIClient
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class TradesIngester:
    """Handles ingestion of trades data from the API."""
    
    def __init__(self):
        """Initialize the trades ingester."""
        self.db_manager = get_db_manager()
        self.api_client = RiskAnalyticsAPIClient()
        
        # Table mapping for trade types
        self.table_mapping = {
            'closed': 'raw_trades_closed',
            'open': 'raw_trades_open'
        }
    
    def ingest_trades(self,
                     trade_type: str,
                     start_date: Optional[date] = None,
                     end_date: Optional[date] = None,
                     logins: Optional[List[str]] = None,
                     symbols: Optional[List[str]] = None,
                     batch_days: int = 7,
                     force_full_refresh: bool = False) -> int:
        """
        Ingest trades data from the API.
        
        Args:
            trade_type: Type of trades ('closed' or 'open')
            start_date: Start date for trade ingestion
            end_date: End date for trade ingestion
            logins: Optional list of specific login IDs
            symbols: Optional list of specific symbols
            batch_days: Number of days to process at once (for closed trades)
            force_full_refresh: If True, truncate existing data and reload
            
        Returns:
            Number of records ingested
        """
        if trade_type not in self.table_mapping:
            raise ValueError(f"Invalid trade type: {trade_type}")
        
        table_name = self.table_mapping[trade_type]
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage=f'ingest_trades_{trade_type}',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Handle full refresh
            if force_full_refresh:
                logger.warning(f"Force full refresh requested for {trade_type} trades. Truncating existing data.")
                self.db_manager.model_db.execute_command(f"TRUNCATE TABLE {table_name}")
            
            # Determine date range
            if not end_date:
                end_date = datetime.now().date() - timedelta(days=1)  # Yesterday
            if not start_date:
                if trade_type == 'open':
                    # For open trades, we only need recent data
                    start_date = end_date
                else:
                    # For closed trades, default to last 30 days for initial load
                    start_date = end_date - timedelta(days=30)
            
            # Ingest trades
            if trade_type == 'closed':
                total_records = self._ingest_closed_trades(
                    table_name, start_date, end_date, logins, symbols, batch_days
                )
            else:
                total_records = self._ingest_open_trades(
                    table_name, start_date, end_date, logins, symbols
                )
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage=f'ingest_trades_{trade_type}',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'start_date': str(start_date),
                    'end_date': str(end_date),
                    'batch_days': batch_days if trade_type == 'closed' else None,
                    'force_full_refresh': force_full_refresh
                }
            )
            
            logger.info(f"Successfully ingested {total_records} {trade_type} trade records")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage=f'ingest_trades_{trade_type}',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to ingest {trade_type} trades: {str(e)}")
            raise
    
    def _ingest_closed_trades(self,
                            table_name: str,
                            start_date: date,
                            end_date: date,
                            logins: Optional[List[str]],
                            symbols: Optional[List[str]],
                            batch_days: int) -> int:
        """
        Ingest closed trades data in date batches to handle large volume.
        
        Critical: 81M records require careful handling
        """
        batch_data = []
        batch_size = 5000  # Larger batch for closed trades
        total_records = 0
        
        # Process in date chunks to manage volume
        current_start = start_date
        while current_start <= end_date:
            current_end = min(current_start + timedelta(days=batch_days - 1), end_date)
            
            logger.info(f"Processing closed trades from {current_start} to {current_end}")
            
            # Format dates for API
            start_str = self.api_client.format_date(current_start)
            end_str = self.api_client.format_date(current_end)
            
            # Fetch trades for this date range
            for page_num, trades_page in enumerate(self.api_client.get_trades(
                trade_type='closed',
                logins=logins,
                symbols=symbols,
                trade_date_from=start_str,
                trade_date_to=end_str
            )):
                if page_num % 10 == 0:  # Log progress every 10 pages
                    logger.info(f"Processing page {page_num + 1} for dates {start_str}-{end_str}")
                
                for trade in trades_page:
                    record = self._transform_closed_trade(trade)
                    batch_data.append(record)
                    
                    if len(batch_data) >= batch_size:
                        self._insert_trades_batch(batch_data, table_name)
                        total_records += len(batch_data)
                        batch_data = []
                        
                        # Log progress for large datasets
                        if total_records % 50000 == 0:
                            logger.info(f"Progress: {total_records:,} records processed")
            
            # Move to next date batch
            current_start = current_end + timedelta(days=1)
        
        # Insert remaining records
        if batch_data:
            self._insert_trades_batch(batch_data, table_name)
            total_records += len(batch_data)
        
        return total_records
    
    def _ingest_open_trades(self,
                          table_name: str,
                          start_date: date,
                          end_date: date,
                          logins: Optional[List[str]],
                          symbols: Optional[List[str]]) -> int:
        """Ingest open trades data (typically much smaller volume)."""
        batch_data = []
        batch_size = 1000
        total_records = 0
        
        # For open trades, we typically only need the latest snapshot
        logger.info(f"Processing open trades for {end_date}")
        
        # Format date for API
        date_str = self.api_client.format_date(end_date)
        
        for page_num, trades_page in enumerate(self.api_client.get_trades(
            trade_type='open',
            logins=logins,
            symbols=symbols,
            trade_date_from=date_str,
            trade_date_to=date_str
        )):
            logger.info(f"Processing page {page_num + 1} with {len(trades_page)} open trades")
            
            for trade in trades_page:
                record = self._transform_open_trade(trade)
                batch_data.append(record)
                
                if len(batch_data) >= batch_size:
                    self._insert_trades_batch(batch_data, table_name)
                    total_records += len(batch_data)
                    batch_data = []
        
        # Insert remaining records
        if batch_data:
            self._insert_trades_batch(batch_data, table_name)
            total_records += len(batch_data)
        
        return total_records
    
    def _transform_closed_trade(self, trade: Dict[str, Any]) -> Dict[str, Any]:
        """Transform closed trade record for database."""
        # Parse timestamps
        open_time = self._parse_timestamp(trade.get('openTime'))
        close_time = self._parse_timestamp(trade.get('closeTime'))
        
        # Parse trade date from YYYYMMDD format
        trade_date_str = str(trade.get('tradeDate', ''))
        if trade_date_str and len(trade_date_str) == 8:
            trade_date = datetime.strptime(trade_date_str, '%Y%m%d').date()
        else:
            trade_date = None
        
        return {
            'trade_id': trade.get('tradeId'),
            'account_id': trade.get('accountId'),
            'login': trade.get('login'),
            'symbol': trade.get('symbol'),
            'std_symbol': trade.get('stdSymbol'),
            'side': trade.get('side'),
            'open_time': open_time,
            'close_time': close_time,
            'trade_date': trade_date,
            'open_price': trade.get('openPrice'),
            'close_price': trade.get('closePrice'),
            'stop_loss': trade.get('stopLoss'),
            'take_profit': trade.get('takeProfit'),
            'lots': trade.get('lots'),
            'volume_usd': trade.get('volumeUSD'),
            'profit': trade.get('profit'),
            'commission': trade.get('commission'),
            'swap': trade.get('swap'),
            'ingestion_timestamp': datetime.now(),
            'source_api_endpoint': '/v2/trades/closed'
        }
    
    def _transform_open_trade(self, trade: Dict[str, Any]) -> Dict[str, Any]:
        """Transform open trade record for database."""
        # Parse timestamps
        open_time = self._parse_timestamp(trade.get('openTime'))
        
        # Parse trade date
        trade_date_str = str(trade.get('tradeDate', ''))
        if trade_date_str and len(trade_date_str) == 8:
            trade_date = datetime.strptime(trade_date_str, '%Y%m%d').date()
        else:
            trade_date = None
        
        return {
            'trade_id': trade.get('tradeId'),
            'account_id': trade.get('accountId'),
            'login': trade.get('login'),
            'symbol': trade.get('symbol'),
            'std_symbol': trade.get('stdSymbol'),
            'side': trade.get('side'),
            'open_time': open_time,
            'trade_date': trade_date,
            'open_price': trade.get('openPrice'),
            'current_price': trade.get('currentPrice'),
            'stop_loss': trade.get('stopLoss'),
            'take_profit': trade.get('takeProfit'),
            'lots': trade.get('lots'),
            'volume_usd': trade.get('volumeUSD'),
            'unrealized_pnl': trade.get('unrealizedPnL'),
            'commission': trade.get('commission'),
            'swap': trade.get('swap'),
            'ingestion_timestamp': datetime.now(),
            'source_api_endpoint': '/v2/trades/open'
        }
    
    def _parse_timestamp(self, timestamp_str: Optional[str]) -> Optional[datetime]:
        """Parse various timestamp formats from the API."""
        if not timestamp_str:
            return None
        
        # Try different formats
        formats = [
            '%Y-%m-%dT%H:%M:%S.%fZ',
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y-%m-%d %H:%M:%S',
            '%Y%m%d%H%M%S'
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(timestamp_str, fmt)
            except ValueError:
                continue
        
        logger.warning(f"Could not parse timestamp: {timestamp_str}")
        return None
    
    def _insert_trades_batch(self, batch_data: List[Dict[str, Any]], table_name: str):
        """Insert a batch of trade records into the database."""
        if not batch_data:
            return
        
        try:
            # Build the insert query
            columns = list(batch_data[0].keys())
            placeholders = ', '.join(['%s'] * len(columns))
            columns_str = ', '.join(columns)
            
            query = f"""
            INSERT INTO {table_name} ({columns_str})
            VALUES ({placeholders})
            """
            
            # Note: For trades, we don't use ON CONFLICT as trade_id should be unique
            # If duplicates occur, it's better to know about them
            
            # Convert to list of tuples
            values = [tuple(record[col] for col in columns) for record in batch_data]
            
            with self.db_manager.model_db.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.executemany(query, values)
            
            logger.debug(f"Inserted batch of {len(batch_data)} records into {table_name}")
            
        except Exception as e:
            logger.error(f"Failed to insert batch into {table_name}: {str(e)}")
            # Log sample of problematic data for debugging
            if batch_data:
                logger.error(f"Sample record: {batch_data[0]}")
            raise
    
    def close(self):
        """Clean up resources."""
        self.api_client.close()


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Ingest trades data from Risk Analytics API')
    parser.add_argument('trade_type', choices=['closed', 'open'],
                       help='Type of trades to ingest')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Start date for trades (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='End date for trades (YYYY-MM-DD)')
    parser.add_argument('--logins', nargs='+', help='Specific login IDs to fetch')
    parser.add_argument('--symbols', nargs='+', help='Specific symbols to fetch')
    parser.add_argument('--batch-days', type=int, default=7,
                       help='Number of days to process at once for closed trades')
    parser.add_argument('--force-refresh', action='store_true',
                       help='Force full refresh (truncate and reload)')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file=f'ingest_trades_{args.trade_type}')
    
    # Run ingestion
    ingester = TradesIngester()
    try:
        records = ingester.ingest_trades(
            trade_type=args.trade_type,
            start_date=args.start_date,
            end_date=args.end_date,
            logins=args.logins,
            symbols=args.symbols,
            batch_days=args.batch_days,
            force_full_refresh=args.force_refresh
        )
        logger.info(f"Ingestion complete. Total records: {records}")
    finally:
        ingester.close()


if __name__ == '__main__':
    main()

================
File: src/db_schema/schema.sql
================
-- Prop Trading Model Database Schema
-- This schema defines all tables for the daily profit prediction model
-- as specified in baseline-implementation-roadmap.md

-- Create the dedicated schema for the model
CREATE SCHEMA IF NOT EXISTS prop_trading_model;

-- Set the search path to our schema
SET search_path TO prop_trading_model;

-- ========================================
-- Raw Data Tables (Data Ingestion Layer)
-- ========================================

-- Raw accounts data from /accounts API
CREATE TABLE IF NOT EXISTS raw_accounts_data (
    id SERIAL PRIMARY KEY,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    trader_id VARCHAR(255),
    plan_id VARCHAR(255),
    starting_balance DECIMAL(18, 2),
    current_balance DECIMAL(18, 2),
    current_equity DECIMAL(18, 2),
    profit_target_pct DECIMAL(5, 2),
    max_daily_drawdown_pct DECIMAL(5, 2),
    max_drawdown_pct DECIMAL(5, 2),
    max_leverage DECIMAL(10, 2),
    is_drawdown_relative BOOLEAN,
    breached INTEGER DEFAULT 0,
    is_upgraded INTEGER DEFAULT 0,
    phase VARCHAR(50),
    status VARCHAR(50),
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_api_endpoint VARCHAR(500),
    UNIQUE(account_id, ingestion_timestamp)
);

-- Indexes for raw_accounts_data
CREATE INDEX idx_raw_accounts_account_id ON raw_accounts_data(account_id);
CREATE INDEX idx_raw_accounts_login ON raw_accounts_data(login);
CREATE INDEX idx_raw_accounts_ingestion ON raw_accounts_data(ingestion_timestamp);

-- Raw metrics alltime data from /metrics/alltime API
CREATE TABLE IF NOT EXISTS raw_metrics_alltime (
    id SERIAL PRIMARY KEY,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    net_profit DECIMAL(18, 2),
    gross_profit DECIMAL(18, 2),
    gross_loss DECIMAL(18, 2),
    total_trades INTEGER,
    winning_trades INTEGER,
    losing_trades INTEGER,
    win_rate DECIMAL(5, 2),
    profit_factor DECIMAL(10, 2),
    average_win DECIMAL(18, 2),
    average_loss DECIMAL(18, 2),
    average_rrr DECIMAL(10, 2),
    expectancy DECIMAL(18, 2),
    sharpe_ratio DECIMAL(10, 2),
    sortino_ratio DECIMAL(10, 2),
    max_drawdown DECIMAL(18, 2),
    max_drawdown_pct DECIMAL(5, 2),
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_api_endpoint VARCHAR(500),
    UNIQUE(account_id, ingestion_timestamp)
);

-- Indexes for raw_metrics_alltime
CREATE INDEX idx_raw_metrics_alltime_account_id ON raw_metrics_alltime(account_id);
CREATE INDEX idx_raw_metrics_alltime_login ON raw_metrics_alltime(login);

-- Raw metrics daily data from /metrics/daily API (SOURCE FOR TARGET VARIABLE)
CREATE TABLE IF NOT EXISTS raw_metrics_daily (
    id SERIAL PRIMARY KEY,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    date DATE NOT NULL,
    net_profit DECIMAL(18, 2), -- This is our target variable
    gross_profit DECIMAL(18, 2),
    gross_loss DECIMAL(18, 2),
    total_trades INTEGER,
    winning_trades INTEGER,
    losing_trades INTEGER,
    win_rate DECIMAL(5, 2),
    profit_factor DECIMAL(10, 2),
    lots_traded DECIMAL(18, 4),
    volume_traded DECIMAL(18, 2),
    commission DECIMAL(18, 2),
    swap DECIMAL(18, 2),
    balance_start DECIMAL(18, 2),
    balance_end DECIMAL(18, 2),
    equity_start DECIMAL(18, 2),
    equity_end DECIMAL(18, 2),
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_api_endpoint VARCHAR(500),
    UNIQUE(account_id, date, ingestion_timestamp)
);

-- Indexes for raw_metrics_daily
CREATE INDEX idx_raw_metrics_daily_account_id ON raw_metrics_daily(account_id);
CREATE INDEX idx_raw_metrics_daily_date ON raw_metrics_daily(date);
CREATE INDEX idx_raw_metrics_daily_account_date ON raw_metrics_daily(account_id, date);

-- Raw metrics hourly data from /metrics/hourly API
CREATE TABLE IF NOT EXISTS raw_metrics_hourly (
    id SERIAL PRIMARY KEY,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    date DATE NOT NULL,
    hour INTEGER NOT NULL CHECK (hour >= 0 AND hour <= 23),
    net_profit DECIMAL(18, 2),
    gross_profit DECIMAL(18, 2),
    gross_loss DECIMAL(18, 2),
    total_trades INTEGER,
    winning_trades INTEGER,
    losing_trades INTEGER,
    win_rate DECIMAL(5, 2),
    lots_traded DECIMAL(18, 4),
    volume_traded DECIMAL(18, 2),
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_api_endpoint VARCHAR(500),
    UNIQUE(account_id, date, hour, ingestion_timestamp)
);

-- Indexes for raw_metrics_hourly
CREATE INDEX idx_raw_metrics_hourly_account_id ON raw_metrics_hourly(account_id);
CREATE INDEX idx_raw_metrics_hourly_date_hour ON raw_metrics_hourly(date, hour);

-- Raw trades closed data from /trades/closed API (81M records - needs careful handling)
CREATE TABLE IF NOT EXISTS raw_trades_closed (
    id SERIAL PRIMARY KEY,
    trade_id VARCHAR(255) NOT NULL,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    symbol VARCHAR(50),
    std_symbol VARCHAR(50),
    side VARCHAR(10),
    open_time TIMESTAMP,
    close_time TIMESTAMP,
    trade_date DATE,
    open_price DECIMAL(18, 6),
    close_price DECIMAL(18, 6),
    stop_loss DECIMAL(18, 6),
    take_profit DECIMAL(18, 6),
    lots DECIMAL(18, 4),
    volume_usd DECIMAL(18, 2),
    profit DECIMAL(18, 2),
    commission DECIMAL(18, 2),
    swap DECIMAL(18, 2),
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_api_endpoint VARCHAR(500)
);

-- Indexes for raw_trades_closed (critical for 81M records)
CREATE INDEX idx_raw_trades_closed_account_id ON raw_trades_closed(account_id);
CREATE INDEX idx_raw_trades_closed_trade_date ON raw_trades_closed(trade_date);
CREATE INDEX idx_raw_trades_closed_close_time ON raw_trades_closed(close_time);
CREATE INDEX idx_raw_trades_closed_account_date ON raw_trades_closed(account_id, trade_date);

-- Raw trades open data from /trades/open API
CREATE TABLE IF NOT EXISTS raw_trades_open (
    id SERIAL PRIMARY KEY,
    trade_id VARCHAR(255) NOT NULL,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    symbol VARCHAR(50),
    std_symbol VARCHAR(50),
    side VARCHAR(10),
    open_time TIMESTAMP,
    trade_date DATE,
    open_price DECIMAL(18, 6),
    current_price DECIMAL(18, 6),
    stop_loss DECIMAL(18, 6),
    take_profit DECIMAL(18, 6),
    lots DECIMAL(18, 4),
    volume_usd DECIMAL(18, 2),
    unrealized_pnl DECIMAL(18, 2),
    commission DECIMAL(18, 2),
    swap DECIMAL(18, 2),
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_api_endpoint VARCHAR(500)
);

-- Indexes for raw_trades_open
CREATE INDEX idx_raw_trades_open_account_id ON raw_trades_open(account_id);
CREATE INDEX idx_raw_trades_open_trade_date ON raw_trades_open(trade_date);

-- Raw plans data from Plans CSV files
CREATE TABLE IF NOT EXISTS raw_plans_data (
    id SERIAL PRIMARY KEY,
    plan_id VARCHAR(255) NOT NULL,
    plan_name VARCHAR(255),
    plan_type VARCHAR(100),
    starting_balance DECIMAL(18, 2),
    profit_target DECIMAL(18, 2),
    profit_target_pct DECIMAL(5, 2),
    max_drawdown DECIMAL(18, 2),
    max_drawdown_pct DECIMAL(5, 2),
    max_daily_drawdown DECIMAL(18, 2),
    max_daily_drawdown_pct DECIMAL(5, 2),
    max_leverage DECIMAL(10, 2),
    is_drawdown_relative BOOLEAN,
    min_trading_days INTEGER,
    max_trading_days INTEGER,
    profit_split_pct DECIMAL(5, 2),
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(plan_id, ingestion_timestamp)
);

-- Raw regimes daily data from Supabase public.regimes_daily
CREATE TABLE IF NOT EXISTS raw_regimes_daily (
    id SERIAL PRIMARY KEY,
    date DATE NOT NULL,
    market_news JSONB,
    instruments JSONB,
    country_economic_indicators JSONB,
    news_analysis JSONB,
    summary JSONB,
    vector_daily_regime FLOAT[], -- Array of floats for the regime vector
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(date, ingestion_timestamp)
);

-- Indexes for raw_regimes_daily
CREATE INDEX idx_raw_regimes_daily_date ON raw_regimes_daily(date);
CREATE INDEX idx_raw_regimes_daily_ingestion ON raw_regimes_daily(ingestion_timestamp);

-- ========================================
-- Staging Tables (Preprocessing Layer)
-- ========================================

-- Staging table for daily account snapshots
CREATE TABLE IF NOT EXISTS stg_accounts_daily_snapshots (
    id SERIAL PRIMARY KEY,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    date DATE NOT NULL,
    trader_id VARCHAR(255),
    plan_id VARCHAR(255),
    phase VARCHAR(50),
    status VARCHAR(50),
    starting_balance DECIMAL(18, 2),
    current_balance DECIMAL(18, 2),
    current_equity DECIMAL(18, 2),
    profit_target_pct DECIMAL(5, 2),
    max_daily_drawdown_pct DECIMAL(5, 2),
    max_drawdown_pct DECIMAL(5, 2),
    max_leverage DECIMAL(10, 2),
    is_drawdown_relative BOOLEAN,
    days_since_first_trade INTEGER,
    active_trading_days_count INTEGER,
    distance_to_profit_target DECIMAL(18, 2),
    distance_to_max_drawdown DECIMAL(18, 2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(account_id, date)
);

-- Indexes for stg_accounts_daily_snapshots
CREATE INDEX idx_stg_accounts_daily_account_id ON stg_accounts_daily_snapshots(account_id);
CREATE INDEX idx_stg_accounts_daily_date ON stg_accounts_daily_snapshots(date);
CREATE INDEX idx_stg_accounts_daily_account_date ON stg_accounts_daily_snapshots(account_id, date);

-- ========================================
-- Feature Store Table
-- ========================================

-- Feature store for account daily features (features for day D to predict D+1)
CREATE TABLE IF NOT EXISTS feature_store_account_daily (
    id SERIAL PRIMARY KEY,
    account_id VARCHAR(255) NOT NULL,
    login VARCHAR(255) NOT NULL,
    feature_date DATE NOT NULL, -- Date D (features are for this date)
    
    -- Static Account & Plan Features
    starting_balance DECIMAL(18, 2),
    max_daily_drawdown_pct DECIMAL(5, 2),
    max_drawdown_pct DECIMAL(5, 2),
    profit_target_pct DECIMAL(5, 2),
    max_leverage DECIMAL(10, 2),
    is_drawdown_relative INTEGER,
    
    -- Dynamic Account State Features (as of EOD D)
    current_balance DECIMAL(18, 2),
    current_equity DECIMAL(18, 2),
    days_since_first_trade INTEGER,
    active_trading_days_count INTEGER,
    distance_to_profit_target DECIMAL(18, 2),
    distance_to_max_drawdown DECIMAL(18, 2),
    open_pnl DECIMAL(18, 2),
    open_positions_volume DECIMAL(18, 2),
    
    -- Historical Performance Features (Rolling Windows)
    -- 1-day window
    rolling_pnl_sum_1d DECIMAL(18, 2),
    rolling_pnl_avg_1d DECIMAL(18, 2),
    rolling_pnl_std_1d DECIMAL(18, 2),
    
    -- 3-day window
    rolling_pnl_sum_3d DECIMAL(18, 2),
    rolling_pnl_avg_3d DECIMAL(18, 2),
    rolling_pnl_std_3d DECIMAL(18, 2),
    rolling_pnl_min_3d DECIMAL(18, 2),
    rolling_pnl_max_3d DECIMAL(18, 2),
    win_rate_3d DECIMAL(5, 2),
    
    -- 5-day window
    rolling_pnl_sum_5d DECIMAL(18, 2),
    rolling_pnl_avg_5d DECIMAL(18, 2),
    rolling_pnl_std_5d DECIMAL(18, 2),
    rolling_pnl_min_5d DECIMAL(18, 2),
    rolling_pnl_max_5d DECIMAL(18, 2),
    win_rate_5d DECIMAL(5, 2),
    profit_factor_5d DECIMAL(10, 2),
    sharpe_ratio_5d DECIMAL(10, 2),
    
    -- 10-day window
    rolling_pnl_sum_10d DECIMAL(18, 2),
    rolling_pnl_avg_10d DECIMAL(18, 2),
    rolling_pnl_std_10d DECIMAL(18, 2),
    rolling_pnl_min_10d DECIMAL(18, 2),
    rolling_pnl_max_10d DECIMAL(18, 2),
    win_rate_10d DECIMAL(5, 2),
    profit_factor_10d DECIMAL(10, 2),
    sharpe_ratio_10d DECIMAL(10, 2),
    
    -- 20-day window
    rolling_pnl_sum_20d DECIMAL(18, 2),
    rolling_pnl_avg_20d DECIMAL(18, 2),
    rolling_pnl_std_20d DECIMAL(18, 2),
    win_rate_20d DECIMAL(5, 2),
    profit_factor_20d DECIMAL(10, 2),
    sharpe_ratio_20d DECIMAL(10, 2),
    
    -- Behavioral Features
    trades_count_5d INTEGER,
    avg_trade_duration_5d DECIMAL(10, 2),
    avg_lots_per_trade_5d DECIMAL(18, 4),
    avg_volume_per_trade_5d DECIMAL(18, 2),
    stop_loss_usage_rate_5d DECIMAL(5, 2),
    take_profit_usage_rate_5d DECIMAL(5, 2),
    buy_sell_ratio_5d DECIMAL(5, 2),
    top_symbol_concentration_5d DECIMAL(5, 2),
    
    -- Market Regime Features (from regimes_daily for day D)
    market_sentiment_score DECIMAL(10, 4),
    market_volatility_regime VARCHAR(50),
    market_liquidity_state VARCHAR(50),
    vix_level DECIMAL(10, 2),
    dxy_level DECIMAL(10, 2),
    sp500_daily_return DECIMAL(10, 4),
    btc_volatility_90d DECIMAL(10, 4),
    fed_funds_rate DECIMAL(10, 4),
    
    -- Date and Time Features
    day_of_week INTEGER,
    week_of_month INTEGER,
    month INTEGER,
    quarter INTEGER,
    day_of_year INTEGER,
    is_month_start BOOLEAN,
    is_month_end BOOLEAN,
    is_quarter_start BOOLEAN,
    is_quarter_end BOOLEAN,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(account_id, feature_date)
);

-- Indexes for feature_store_account_daily
CREATE INDEX idx_feature_store_account_id ON feature_store_account_daily(account_id);
CREATE INDEX idx_feature_store_date ON feature_store_account_daily(feature_date);
CREATE INDEX idx_feature_store_account_date ON feature_store_account_daily(account_id, feature_date);

-- ========================================
-- Model Training and Prediction Tables
-- ========================================

-- Model training input table (features from day D, target from day D+1)
CREATE TABLE IF NOT EXISTS model_training_input (
    id SERIAL PRIMARY KEY,
    login VARCHAR(255) NOT NULL,
    prediction_date DATE NOT NULL, -- Date D+1 (date we're predicting for)
    feature_date DATE NOT NULL, -- Date D (date features are from)
    
    -- All features from feature_store_account_daily
    -- (These are duplicated here for training convenience)
    starting_balance DECIMAL(18, 2),
    max_daily_drawdown_pct DECIMAL(5, 2),
    max_drawdown_pct DECIMAL(5, 2),
    profit_target_pct DECIMAL(5, 2),
    max_leverage DECIMAL(10, 2),
    is_drawdown_relative INTEGER,
    current_balance DECIMAL(18, 2),
    current_equity DECIMAL(18, 2),
    days_since_first_trade INTEGER,
    active_trading_days_count INTEGER,
    distance_to_profit_target DECIMAL(18, 2),
    distance_to_max_drawdown DECIMAL(18, 2),
    open_pnl DECIMAL(18, 2),
    open_positions_volume DECIMAL(18, 2),
    rolling_pnl_sum_1d DECIMAL(18, 2),
    rolling_pnl_avg_1d DECIMAL(18, 2),
    rolling_pnl_std_1d DECIMAL(18, 2),
    rolling_pnl_sum_3d DECIMAL(18, 2),
    rolling_pnl_avg_3d DECIMAL(18, 2),
    rolling_pnl_std_3d DECIMAL(18, 2),
    rolling_pnl_min_3d DECIMAL(18, 2),
    rolling_pnl_max_3d DECIMAL(18, 2),
    win_rate_3d DECIMAL(5, 2),
    rolling_pnl_sum_5d DECIMAL(18, 2),
    rolling_pnl_avg_5d DECIMAL(18, 2),
    rolling_pnl_std_5d DECIMAL(18, 2),
    rolling_pnl_min_5d DECIMAL(18, 2),
    rolling_pnl_max_5d DECIMAL(18, 2),
    win_rate_5d DECIMAL(5, 2),
    profit_factor_5d DECIMAL(10, 2),
    sharpe_ratio_5d DECIMAL(10, 2),
    rolling_pnl_sum_10d DECIMAL(18, 2),
    rolling_pnl_avg_10d DECIMAL(18, 2),
    rolling_pnl_std_10d DECIMAL(18, 2),
    rolling_pnl_min_10d DECIMAL(18, 2),
    rolling_pnl_max_10d DECIMAL(18, 2),
    win_rate_10d DECIMAL(5, 2),
    profit_factor_10d DECIMAL(10, 2),
    sharpe_ratio_10d DECIMAL(10, 2),
    rolling_pnl_sum_20d DECIMAL(18, 2),
    rolling_pnl_avg_20d DECIMAL(18, 2),
    rolling_pnl_std_20d DECIMAL(18, 2),
    win_rate_20d DECIMAL(5, 2),
    profit_factor_20d DECIMAL(10, 2),
    sharpe_ratio_20d DECIMAL(10, 2),
    trades_count_5d INTEGER,
    avg_trade_duration_5d DECIMAL(10, 2),
    avg_lots_per_trade_5d DECIMAL(18, 4),
    avg_volume_per_trade_5d DECIMAL(18, 2),
    stop_loss_usage_rate_5d DECIMAL(5, 2),
    take_profit_usage_rate_5d DECIMAL(5, 2),
    buy_sell_ratio_5d DECIMAL(5, 2),
    top_symbol_concentration_5d DECIMAL(5, 2),
    market_sentiment_score DECIMAL(10, 4),
    market_volatility_regime VARCHAR(50),
    market_liquidity_state VARCHAR(50),
    vix_level DECIMAL(10, 2),
    dxy_level DECIMAL(10, 2),
    sp500_daily_return DECIMAL(10, 4),
    btc_volatility_90d DECIMAL(10, 4),
    fed_funds_rate DECIMAL(10, 4),
    day_of_week INTEGER,
    week_of_month INTEGER,
    month INTEGER,
    quarter INTEGER,
    day_of_year INTEGER,
    is_month_start BOOLEAN,
    is_month_end BOOLEAN,
    is_quarter_start BOOLEAN,
    is_quarter_end BOOLEAN,
    
    -- Target variable (from raw_metrics_daily for prediction_date)
    target_net_profit DECIMAL(18, 2),
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(login, prediction_date)
);

-- Indexes for model_training_input
CREATE INDEX idx_model_training_login ON model_training_input(login);
CREATE INDEX idx_model_training_prediction_date ON model_training_input(prediction_date);
CREATE INDEX idx_model_training_feature_date ON model_training_input(feature_date);

-- Model predictions table
CREATE TABLE IF NOT EXISTS model_predictions (
    id SERIAL PRIMARY KEY,
    login VARCHAR(255) NOT NULL,
    prediction_date DATE NOT NULL, -- Date D+1 (date we're predicting for)
    feature_date DATE NOT NULL, -- Date D (date features are from)
    predicted_net_profit DECIMAL(18, 2),
    prediction_confidence DECIMAL(5, 2),
    model_version VARCHAR(50),
    
    -- SHAP values for top features (store as JSONB for flexibility)
    shap_values JSONB,
    top_positive_features JSONB,
    top_negative_features JSONB,
    
    -- Actual outcome (filled in later for evaluation)
    actual_net_profit DECIMAL(18, 2),
    prediction_error DECIMAL(18, 2),
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(login, prediction_date, model_version)
);

-- Indexes for model_predictions
CREATE INDEX idx_model_predictions_login ON model_predictions(login);
CREATE INDEX idx_model_predictions_date ON model_predictions(prediction_date);
CREATE INDEX idx_model_predictions_login_date ON model_predictions(login, prediction_date);

-- ========================================
-- Model Metadata and Versioning
-- ========================================

-- Model registry table
CREATE TABLE IF NOT EXISTS model_registry (
    id SERIAL PRIMARY KEY,
    model_version VARCHAR(50) NOT NULL UNIQUE,
    model_type VARCHAR(50) DEFAULT 'LightGBM',
    training_start_date DATE,
    training_end_date DATE,
    validation_start_date DATE,
    validation_end_date DATE,
    test_start_date DATE,
    test_end_date DATE,
    
    -- Model performance metrics
    train_mae DECIMAL(18, 4),
    train_rmse DECIMAL(18, 4),
    train_r2 DECIMAL(5, 4),
    val_mae DECIMAL(18, 4),
    val_rmse DECIMAL(18, 4),
    val_r2 DECIMAL(5, 4),
    test_mae DECIMAL(18, 4),
    test_rmse DECIMAL(18, 4),
    test_r2 DECIMAL(5, 4),
    
    -- Model parameters (stored as JSONB)
    hyperparameters JSONB,
    feature_list JSONB,
    feature_importance JSONB,
    
    -- Model artifacts location
    model_file_path VARCHAR(500),
    scaler_file_path VARCHAR(500),
    
    is_active BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create update trigger for model_registry
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_model_registry_updated_at BEFORE UPDATE
    ON model_registry FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- ========================================
-- Audit and Logging Tables
-- ========================================

-- Pipeline execution log
CREATE TABLE IF NOT EXISTS pipeline_execution_log (
    id SERIAL PRIMARY KEY,
    pipeline_stage VARCHAR(100) NOT NULL,
    execution_date DATE NOT NULL,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    status VARCHAR(50) NOT NULL, -- 'running', 'success', 'failed'
    records_processed INTEGER,
    error_message TEXT,
    execution_details JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for pipeline_execution_log
CREATE INDEX idx_pipeline_log_stage ON pipeline_execution_log(pipeline_stage);
CREATE INDEX idx_pipeline_log_date ON pipeline_execution_log(execution_date);
CREATE INDEX idx_pipeline_log_status ON pipeline_execution_log(status);

-- ========================================
-- Grant permissions (adjust as needed)
-- ========================================

-- Example: Grant usage on schema to application user
-- GRANT USAGE ON SCHEMA prop_trading_model TO your_app_user;
-- GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA prop_trading_model TO your_app_user;
-- GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA prop_trading_model TO your_app_user;

================
File: src/feature_engineering/__init__.py
================
# Feature engineering modules for the daily profit model

================
File: src/feature_engineering/build_training_data.py
================
"""
Build the model training input table by combining features with target variables.
Features from day D are aligned with target PnL from day D+1.
"""

import os
import sys
import logging
from datetime import datetime, timedelta, date
from typing import Optional
import argparse
import pandas as pd

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class TrainingDataBuilder:
    """Builds the model training input table."""
    
    def __init__(self):
        """Initialize the training data builder."""
        self.db_manager = get_db_manager()
        self.training_table = 'model_training_input'
        
    def build_training_data(self,
                          start_date: Optional[date] = None,
                          end_date: Optional[date] = None,
                          force_rebuild: bool = False) -> int:
        """
        Build training data by aligning features with target variables.
        
        Args:
            start_date: Start date for training data
            end_date: End date for training data
            force_rebuild: If True, rebuild even if data exists
            
        Returns:
            Number of training records created
        """
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='build_training_data',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Determine date range
            if not end_date:
                end_date = datetime.now().date() - timedelta(days=2)  # Need D+1 for target
            if not start_date:
                start_date = end_date - timedelta(days=90)  # Default to 90 days
            
            logger.info(f"Building training data from {start_date} to {end_date}")
            
            # Clear existing data if force rebuild
            if force_rebuild:
                logger.warning("Force rebuild requested. Truncating existing training data.")
                self.db_manager.model_db.execute_command(f"TRUNCATE TABLE {self.training_table}")
            
            # Build training data using SQL
            total_records = self._build_training_records(start_date, end_date)
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='build_training_data',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'start_date': str(start_date),
                    'end_date': str(end_date),
                    'force_rebuild': force_rebuild
                }
            )
            
            logger.info(f"Successfully created {total_records} training records")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='build_training_data',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to build training data: {str(e)}")
            raise
    
    def _build_training_records(self, start_date: date, end_date: date) -> int:
        """
        Build training records by joining features with target variables.
        
        The key alignment:
        - Features are from feature_date (day D)
        - Target is from prediction_date (day D+1)
        """
        # This query joins features from day D with target PnL from day D+1
        insert_query = f"""
        INSERT INTO {self.training_table} (
            login, prediction_date, feature_date,
            -- Static features
            starting_balance, max_daily_drawdown_pct, max_drawdown_pct,
            profit_target_pct, max_leverage, is_drawdown_relative,
            -- Dynamic features
            current_balance, current_equity, days_since_first_trade,
            active_trading_days_count, distance_to_profit_target,
            distance_to_max_drawdown, open_pnl, open_positions_volume,
            -- Rolling performance features
            rolling_pnl_sum_1d, rolling_pnl_avg_1d, rolling_pnl_std_1d,
            rolling_pnl_sum_3d, rolling_pnl_avg_3d, rolling_pnl_std_3d,
            rolling_pnl_min_3d, rolling_pnl_max_3d, win_rate_3d,
            rolling_pnl_sum_5d, rolling_pnl_avg_5d, rolling_pnl_std_5d,
            rolling_pnl_min_5d, rolling_pnl_max_5d, win_rate_5d,
            profit_factor_5d, sharpe_ratio_5d,
            rolling_pnl_sum_10d, rolling_pnl_avg_10d, rolling_pnl_std_10d,
            rolling_pnl_min_10d, rolling_pnl_max_10d, win_rate_10d,
            profit_factor_10d, sharpe_ratio_10d,
            rolling_pnl_sum_20d, rolling_pnl_avg_20d, rolling_pnl_std_20d,
            win_rate_20d, profit_factor_20d, sharpe_ratio_20d,
            -- Behavioral features
            trades_count_5d, avg_trade_duration_5d, avg_lots_per_trade_5d,
            avg_volume_per_trade_5d, stop_loss_usage_rate_5d,
            take_profit_usage_rate_5d, buy_sell_ratio_5d,
            top_symbol_concentration_5d,
            -- Market features
            market_sentiment_score, market_volatility_regime,
            market_liquidity_state, vix_level, dxy_level,
            sp500_daily_return, btc_volatility_90d, fed_funds_rate,
            -- Time features
            day_of_week, week_of_month, month, quarter, day_of_year,
            is_month_start, is_month_end, is_quarter_start, is_quarter_end,
            -- Target variable
            target_net_profit
        )
        SELECT 
            f.login,
            f.feature_date + INTERVAL '1 day' as prediction_date,  -- D+1
            f.feature_date,  -- D
            -- All feature columns
            f.starting_balance, f.max_daily_drawdown_pct, f.max_drawdown_pct,
            f.profit_target_pct, f.max_leverage, f.is_drawdown_relative,
            f.current_balance, f.current_equity, f.days_since_first_trade,
            f.active_trading_days_count, f.distance_to_profit_target,
            f.distance_to_max_drawdown, f.open_pnl, f.open_positions_volume,
            f.rolling_pnl_sum_1d, f.rolling_pnl_avg_1d, f.rolling_pnl_std_1d,
            f.rolling_pnl_sum_3d, f.rolling_pnl_avg_3d, f.rolling_pnl_std_3d,
            f.rolling_pnl_min_3d, f.rolling_pnl_max_3d, f.win_rate_3d,
            f.rolling_pnl_sum_5d, f.rolling_pnl_avg_5d, f.rolling_pnl_std_5d,
            f.rolling_pnl_min_5d, f.rolling_pnl_max_5d, f.win_rate_5d,
            f.profit_factor_5d, f.sharpe_ratio_5d,
            f.rolling_pnl_sum_10d, f.rolling_pnl_avg_10d, f.rolling_pnl_std_10d,
            f.rolling_pnl_min_10d, f.rolling_pnl_max_10d, f.win_rate_10d,
            f.profit_factor_10d, f.sharpe_ratio_10d,
            f.rolling_pnl_sum_20d, f.rolling_pnl_avg_20d, f.rolling_pnl_std_20d,
            f.win_rate_20d, f.profit_factor_20d, f.sharpe_ratio_20d,
            f.trades_count_5d, f.avg_trade_duration_5d, f.avg_lots_per_trade_5d,
            f.avg_volume_per_trade_5d, f.stop_loss_usage_rate_5d,
            f.take_profit_usage_rate_5d, f.buy_sell_ratio_5d,
            f.top_symbol_concentration_5d,
            f.market_sentiment_score, f.market_volatility_regime,
            f.market_liquidity_state, f.vix_level, f.dxy_level,
            f.sp500_daily_return, f.btc_volatility_90d, f.fed_funds_rate,
            f.day_of_week, f.week_of_month, f.month, f.quarter, f.day_of_year,
            f.is_month_start, f.is_month_end, f.is_quarter_start, f.is_quarter_end,
            -- Target from D+1
            COALESCE(t.net_profit, 0) as target_net_profit
        FROM feature_store_account_daily f
        LEFT JOIN raw_metrics_daily t
            ON f.account_id = t.account_id 
            AND t.date = f.feature_date + INTERVAL '1 day'
        WHERE f.feature_date >= %s 
            AND f.feature_date <= %s
            AND f.account_id IN (
                -- Only include accounts that are active on D+1
                SELECT DISTINCT account_id 
                FROM stg_accounts_daily_snapshots
                WHERE date = f.feature_date + INTERVAL '1 day'
            )
        ON CONFLICT (login, prediction_date) DO NOTHING
        """
        
        # Execute the insert
        rows_affected = self.db_manager.model_db.execute_command(
            insert_query, (start_date, end_date)
        )
        
        return rows_affected
    
    def validate_training_data(self) -> Dict[str, Any]:
        """Validate the training data for completeness and quality."""
        validation_results = {}
        
        # Check record count
        count_query = f"SELECT COUNT(*) as total FROM {self.training_table}"
        result = self.db_manager.model_db.execute_query(count_query)
        validation_results['total_records'] = result[0]['total'] if result else 0
        
        # Check for NULL targets
        null_target_query = f"""
        SELECT COUNT(*) as null_targets 
        FROM {self.training_table}
        WHERE target_net_profit IS NULL
        """
        result = self.db_manager.model_db.execute_query(null_target_query)
        validation_results['null_targets'] = result[0]['null_targets'] if result else 0
        
        # Check date alignment
        alignment_query = f"""
        SELECT 
            MIN(prediction_date - feature_date) as min_diff,
            MAX(prediction_date - feature_date) as max_diff,
            AVG(prediction_date - feature_date) as avg_diff
        FROM {self.training_table}
        """
        result = self.db_manager.model_db.execute_query(alignment_query)
        if result:
            validation_results['date_alignment'] = {
                'min_diff_days': result[0]['min_diff'].days if result[0]['min_diff'] else None,
                'max_diff_days': result[0]['max_diff'].days if result[0]['max_diff'] else None,
                'avg_diff_days': float(result[0]['avg_diff'].days) if result[0]['avg_diff'] else None
            }
        
        # Check feature completeness
        feature_query = f"""
        SELECT 
            COUNT(*) as total,
            COUNT(current_balance) as has_balance,
            COUNT(rolling_pnl_avg_5d) as has_rolling_features,
            COUNT(market_sentiment_score) as has_market_features
        FROM {self.training_table}
        """
        result = self.db_manager.model_db.execute_query(feature_query)
        if result:
            total = result[0]['total']
            validation_results['feature_completeness'] = {
                'balance_coverage': (result[0]['has_balance'] / total * 100) if total > 0 else 0,
                'rolling_coverage': (result[0]['has_rolling_features'] / total * 100) if total > 0 else 0,
                'market_coverage': (result[0]['has_market_features'] / total * 100) if total > 0 else 0
            }
        
        # Target variable statistics
        target_stats_query = f"""
        SELECT 
            AVG(target_net_profit) as mean_pnl,
            STDDEV(target_net_profit) as std_pnl,
            MIN(target_net_profit) as min_pnl,
            MAX(target_net_profit) as max_pnl,
            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY target_net_profit) as median_pnl,
            COUNT(CASE WHEN target_net_profit > 0 THEN 1 END)::FLOAT / COUNT(*) * 100 as win_rate
        FROM {self.training_table}
        WHERE target_net_profit IS NOT NULL
        """
        result = self.db_manager.model_db.execute_query(target_stats_query)
        if result:
            validation_results['target_statistics'] = result[0]
        
        return validation_results


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Build model training data')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Start date for training data (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='End date for training data (YYYY-MM-DD)')
    parser.add_argument('--force-rebuild', action='store_true',
                       help='Force rebuild of existing training data')
    parser.add_argument('--validate', action='store_true',
                       help='Validate training data after building')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='build_training_data')
    
    # Run training data builder
    builder = TrainingDataBuilder()
    try:
        records = builder.build_training_data(
            start_date=args.start_date,
            end_date=args.end_date,
            force_rebuild=args.force_rebuild
        )
        logger.info(f"Training data build complete. Total records: {records}")
        
        # Validate if requested
        if args.validate:
            logger.info("Validating training data...")
            validation = builder.validate_training_data()
            
            logger.info(f"Total records: {validation['total_records']}")
            logger.info(f"NULL targets: {validation['null_targets']}")
            
            if 'date_alignment' in validation:
                logger.info(f"Date alignment: {validation['date_alignment']}")
            
            if 'feature_completeness' in validation:
                logger.info(f"Feature completeness: {validation['feature_completeness']}")
            
            if 'target_statistics' in validation:
                stats = validation['target_statistics']
                logger.info(f"Target PnL statistics:")
                logger.info(f"  - Mean: ${stats['mean_pnl']:.2f}")
                logger.info(f"  - Std Dev: ${stats['std_pnl']:.2f}")
                logger.info(f"  - Min: ${stats['min_pnl']:.2f}")
                logger.info(f"  - Max: ${stats['max_pnl']:.2f}")
                logger.info(f"  - Median: ${stats['median_pnl']:.2f}")
                logger.info(f"  - Win Rate: {stats['win_rate']:.2f}%")
        
    except Exception as e:
        logger.error(f"Training data build failed: {str(e)}")
        raise


if __name__ == '__main__':
    main()

================
File: src/feature_engineering/engineer_features.py
================
"""
Engineer features for the daily profit prediction model.
Creates features from multiple data sources and stores them in feature_store_account_daily.
"""

import os
import sys
import logging
from datetime import datetime, timedelta, date
from typing import Dict, List, Any, Optional, Tuple
import argparse
import pandas as pd
import numpy as np
import json
from scipy import stats

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class FeatureEngineer:
    """Handles feature engineering for the daily profit model."""
    
    def __init__(self):
        """Initialize the feature engineer."""
        self.db_manager = get_db_manager()
        self.feature_table = 'feature_store_account_daily'
        
        # Rolling window configurations
        self.rolling_windows = [1, 3, 5, 10, 20]
        
    def engineer_features(self,
                         start_date: Optional[date] = None,
                         end_date: Optional[date] = None,
                         force_rebuild: bool = False) -> int:
        """
        Engineer features for all accounts and dates in the specified range.
        
        Args:
            start_date: Start date for feature engineering
            end_date: End date for feature engineering
            force_rebuild: If True, rebuild features even if they exist
            
        Returns:
            Number of feature records created
        """
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='engineer_features',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Determine date range
            if not end_date:
                end_date = datetime.now().date() - timedelta(days=1)
            if not start_date:
                start_date = end_date - timedelta(days=30)
            
            logger.info(f"Engineering features from {start_date} to {end_date}")
            
            # Get all unique accounts
            accounts = self._get_active_accounts(start_date, end_date)
            logger.info(f"Found {len(accounts)} active accounts to process")
            
            # Process each account
            for account_id, login in accounts:
                account_records = self._engineer_features_for_account(
                    account_id, login, start_date, end_date, force_rebuild
                )
                total_records += account_records
                
                if total_records % 1000 == 0:
                    logger.info(f"Progress: {total_records} feature records created")
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='engineer_features',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'start_date': str(start_date),
                    'end_date': str(end_date),
                    'accounts_processed': len(accounts),
                    'force_rebuild': force_rebuild
                }
            )
            
            logger.info(f"Successfully created {total_records} feature records")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='engineer_features',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to engineer features: {str(e)}")
            raise
    
    def _get_active_accounts(self, start_date: date, end_date: date) -> List[Tuple[str, str]]:
        """Get list of active accounts in the date range."""
        query = """
        SELECT DISTINCT account_id, login
        FROM stg_accounts_daily_snapshots
        WHERE date >= %s AND date <= %s
        ORDER BY account_id
        """
        results = self.db_manager.model_db.execute_query(query, (start_date, end_date))
        return [(r['account_id'], r['login']) for r in results]
    
    def _engineer_features_for_account(self,
                                     account_id: str,
                                     login: str,
                                     start_date: date,
                                     end_date: date,
                                     force_rebuild: bool) -> int:
        """Engineer features for a single account over the date range."""
        records_created = 0
        
        # Process each date
        current_date = start_date
        while current_date <= end_date:
            # Check if features already exist
            if not force_rebuild and self._features_exist(account_id, current_date):
                current_date += timedelta(days=1)
                continue
            
            # Engineer features for this date
            features = self._calculate_features_for_date(account_id, login, current_date)
            
            if features:
                self._save_features(features)
                records_created += 1
            
            current_date += timedelta(days=1)
        
        return records_created
    
    def _features_exist(self, account_id: str, feature_date: date) -> bool:
        """Check if features already exist for an account and date."""
        query = f"""
        SELECT EXISTS(
            SELECT 1 FROM {self.feature_table}
            WHERE account_id = %s AND feature_date = %s
        )
        """
        result = self.db_manager.model_db.execute_query(query, (account_id, feature_date))
        return result[0]['exists'] if result else False
    
    def _calculate_features_for_date(self,
                                   account_id: str,
                                   login: str,
                                   feature_date: date) -> Optional[Dict[str, Any]]:
        """
        Calculate all features for an account on a specific date.
        Features from day D are used to predict PnL for day D+1.
        """
        try:
            # Initialize feature dictionary
            features = {
                'account_id': account_id,
                'login': login,
                'feature_date': feature_date
            }
            
            # 1. Static Account & Plan Features
            static_features = self._get_static_features(account_id, feature_date)
            features.update(static_features)
            
            # 2. Dynamic Account State Features
            dynamic_features = self._get_dynamic_features(account_id, feature_date)
            features.update(dynamic_features)
            
            # 3. Historical Performance Features
            performance_features = self._get_performance_features(account_id, feature_date)
            features.update(performance_features)
            
            # 4. Behavioral Features
            behavioral_features = self._get_behavioral_features(account_id, feature_date)
            features.update(behavioral_features)
            
            # 5. Market Regime Features
            market_features = self._get_market_features(feature_date)
            features.update(market_features)
            
            # 6. Date and Time Features
            time_features = self._get_time_features(feature_date)
            features.update(time_features)
            
            return features
            
        except Exception as e:
            logger.error(f"Error calculating features for {account_id} on {feature_date}: {str(e)}")
            return None
    
    def _get_static_features(self, account_id: str, feature_date: date) -> Dict[str, Any]:
        """Get static account and plan features."""
        query = """
        SELECT 
            starting_balance,
            max_daily_drawdown_pct,
            max_drawdown_pct,
            profit_target_pct,
            max_leverage,
            CASE WHEN is_drawdown_relative THEN 1 ELSE 0 END as is_drawdown_relative
        FROM stg_accounts_daily_snapshots
        WHERE account_id = %s AND date = %s
        """
        
        result = self.db_manager.model_db.execute_query(query, (account_id, feature_date))
        
        if result:
            return result[0]
        else:
            # Return defaults if not found
            return {
                'starting_balance': None,
                'max_daily_drawdown_pct': None,
                'max_drawdown_pct': None,
                'profit_target_pct': None,
                'max_leverage': None,
                'is_drawdown_relative': 0
            }
    
    def _get_dynamic_features(self, account_id: str, feature_date: date) -> Dict[str, Any]:
        """Get dynamic account state features as of EOD feature_date."""
        # Get snapshot data
        snapshot_query = """
        SELECT 
            current_balance,
            current_equity,
            days_since_first_trade,
            active_trading_days_count,
            distance_to_profit_target,
            distance_to_max_drawdown
        FROM stg_accounts_daily_snapshots
        WHERE account_id = %s AND date = %s
        """
        
        snapshot_result = self.db_manager.model_db.execute_query(
            snapshot_query, (account_id, feature_date)
        )
        
        features = {}
        if snapshot_result:
            features.update(snapshot_result[0])
        else:
            features.update({
                'current_balance': None,
                'current_equity': None,
                'days_since_first_trade': 0,
                'active_trading_days_count': 0,
                'distance_to_profit_target': None,
                'distance_to_max_drawdown': None
            })
        
        # Get open positions data
        open_positions_query = """
        SELECT 
            SUM(unrealized_pnl) as open_pnl,
            SUM(volume_usd) as open_positions_volume
        FROM raw_trades_open
        WHERE account_id = %s AND trade_date = %s
        """
        
        open_result = self.db_manager.model_db.execute_query(
            open_positions_query, (account_id, feature_date)
        )
        
        if open_result and open_result[0]['open_pnl'] is not None:
            features['open_pnl'] = open_result[0]['open_pnl']
            features['open_positions_volume'] = open_result[0]['open_positions_volume']
        else:
            features['open_pnl'] = 0.0
            features['open_positions_volume'] = 0.0
        
        return features
    
    def _get_performance_features(self, account_id: str, feature_date: date) -> Dict[str, Any]:
        """Calculate rolling window performance features."""
        features = {}
        
        # Get historical daily PnL data
        pnl_query = """
        SELECT date, net_profit
        FROM raw_metrics_daily
        WHERE account_id = %s AND date <= %s
        ORDER BY date DESC
        LIMIT 60  -- Maximum window we need
        """
        
        pnl_df = self.db_manager.model_db.execute_query_df(pnl_query, (account_id, feature_date))
        
        if pnl_df.empty:
            # Return default values for all rolling features
            for window in self.rolling_windows:
                features.update(self._get_default_rolling_features(window))
            return features
        
        # Calculate features for each rolling window
        for window in self.rolling_windows:
            if len(pnl_df) >= window:
                window_data = pnl_df.head(window)
                
                # Basic statistics
                features[f'rolling_pnl_sum_{window}d'] = window_data['net_profit'].sum()
                features[f'rolling_pnl_avg_{window}d'] = window_data['net_profit'].mean()
                features[f'rolling_pnl_std_{window}d'] = window_data['net_profit'].std()
                
                if window >= 3:
                    features[f'rolling_pnl_min_{window}d'] = window_data['net_profit'].min()
                    features[f'rolling_pnl_max_{window}d'] = window_data['net_profit'].max()
                    
                    # Win rate
                    wins = (window_data['net_profit'] > 0).sum()
                    features[f'win_rate_{window}d'] = (wins / window) * 100
                
                if window >= 5:
                    # Profit factor
                    gains = window_data[window_data['net_profit'] > 0]['net_profit'].sum()
                    losses = abs(window_data[window_data['net_profit'] < 0]['net_profit'].sum())
                    if losses > 0:
                        features[f'profit_factor_{window}d'] = gains / losses
                    else:
                        features[f'profit_factor_{window}d'] = gains if gains > 0 else 0
                    
                    # Sharpe ratio (simplified)
                    if features[f'rolling_pnl_std_{window}d'] > 0:
                        features[f'sharpe_ratio_{window}d'] = (
                            features[f'rolling_pnl_avg_{window}d'] / 
                            features[f'rolling_pnl_std_{window}d']
                        ) * np.sqrt(252)  # Annualized
                    else:
                        features[f'sharpe_ratio_{window}d'] = 0
            else:
                features.update(self._get_default_rolling_features(window))
        
        return features
    
    def _get_default_rolling_features(self, window: int) -> Dict[str, float]:
        """Get default values for rolling features when not enough data."""
        features = {
            f'rolling_pnl_sum_{window}d': 0.0,
            f'rolling_pnl_avg_{window}d': 0.0,
            f'rolling_pnl_std_{window}d': 0.0
        }
        
        if window >= 3:
            features[f'rolling_pnl_min_{window}d'] = 0.0
            features[f'rolling_pnl_max_{window}d'] = 0.0
            features[f'win_rate_{window}d'] = 0.0
        
        if window >= 5:
            features[f'profit_factor_{window}d'] = 0.0
            features[f'sharpe_ratio_{window}d'] = 0.0
        
        return features
    
    def _get_behavioral_features(self, account_id: str, feature_date: date) -> Dict[str, Any]:
        """Calculate behavioral trading features."""
        # Use 5-day window for behavioral features
        window_start = feature_date - timedelta(days=4)
        
        trades_query = """
        SELECT 
            trade_id,
            symbol,
            std_symbol,
            side,
            open_time,
            close_time,
            stop_loss,
            take_profit,
            lots,
            volume_usd
        FROM raw_trades_closed
        WHERE account_id = %s 
            AND trade_date >= %s 
            AND trade_date <= %s
        """
        
        trades_df = self.db_manager.model_db.execute_query_df(
            trades_query, (account_id, window_start, feature_date)
        )
        
        features = {}
        
        if trades_df.empty:
            # Return default values
            features.update({
                'trades_count_5d': 0,
                'avg_trade_duration_5d': 0.0,
                'avg_lots_per_trade_5d': 0.0,
                'avg_volume_per_trade_5d': 0.0,
                'stop_loss_usage_rate_5d': 0.0,
                'take_profit_usage_rate_5d': 0.0,
                'buy_sell_ratio_5d': 0.5,
                'top_symbol_concentration_5d': 0.0
            })
        else:
            # Trade count
            features['trades_count_5d'] = len(trades_df)
            
            # Average trade duration (in hours)
            if 'open_time' in trades_df.columns and 'close_time' in trades_df.columns:
                trades_df['duration'] = (
                    pd.to_datetime(trades_df['close_time']) - 
                    pd.to_datetime(trades_df['open_time'])
                ).dt.total_seconds() / 3600
                features['avg_trade_duration_5d'] = trades_df['duration'].mean()
            else:
                features['avg_trade_duration_5d'] = 0.0
            
            # Average lots and volume
            features['avg_lots_per_trade_5d'] = trades_df['lots'].mean()
            features['avg_volume_per_trade_5d'] = trades_df['volume_usd'].mean()
            
            # Stop loss and take profit usage
            sl_count = (trades_df['stop_loss'].notna() & (trades_df['stop_loss'] != 0)).sum()
            tp_count = (trades_df['take_profit'].notna() & (trades_df['take_profit'] != 0)).sum()
            features['stop_loss_usage_rate_5d'] = (sl_count / len(trades_df)) * 100
            features['take_profit_usage_rate_5d'] = (tp_count / len(trades_df)) * 100
            
            # Buy/sell ratio
            buy_count = (trades_df['side'] == 'buy').sum()
            sell_count = (trades_df['side'] == 'sell').sum()
            total_sides = buy_count + sell_count
            if total_sides > 0:
                features['buy_sell_ratio_5d'] = buy_count / total_sides
            else:
                features['buy_sell_ratio_5d'] = 0.5
            
            # Symbol concentration
            if 'std_symbol' in trades_df.columns:
                symbol_counts = trades_df['std_symbol'].value_counts()
                if len(symbol_counts) > 0:
                    features['top_symbol_concentration_5d'] = (
                        symbol_counts.iloc[0] / len(trades_df)
                    ) * 100
                else:
                    features['top_symbol_concentration_5d'] = 0.0
            else:
                features['top_symbol_concentration_5d'] = 0.0
        
        return features
    
    def _get_market_features(self, feature_date: date) -> Dict[str, Any]:
        """Extract market regime features for the date."""
        query = """
        SELECT 
            market_news,
            instruments,
            country_economic_indicators,
            news_analysis,
            summary
        FROM raw_regimes_daily
        WHERE date = %s
        ORDER BY ingestion_timestamp DESC
        LIMIT 1
        """
        
        result = self.db_manager.model_db.execute_query(query, (feature_date,))
        
        features = {}
        
        if result:
            regime_data = result[0]
            
            # Parse sentiment score
            try:
                news_analysis = json.loads(regime_data['news_analysis']) if isinstance(
                    regime_data['news_analysis'], str
                ) else regime_data['news_analysis']
                
                sentiment_score = news_analysis.get('sentiment_summary', {}).get(
                    'average_score', 0.0
                )
                features['market_sentiment_score'] = sentiment_score
            except:
                features['market_sentiment_score'] = 0.0
            
            # Parse volatility regime and liquidity state
            try:
                summary = json.loads(regime_data['summary']) if isinstance(
                    regime_data['summary'], str
                ) else regime_data['summary']
                
                key_metrics = summary.get('key_metrics', {})
                features['market_volatility_regime'] = key_metrics.get('volatility_regime', 'normal')
                features['market_liquidity_state'] = key_metrics.get('liquidity_state', 'normal')
            except:
                features['market_volatility_regime'] = 'normal'
                features['market_liquidity_state'] = 'normal'
            
            # Parse instrument data
            try:
                instruments = json.loads(regime_data['instruments']) if isinstance(
                    regime_data['instruments'], str
                ) else regime_data['instruments']
                
                # Get specific asset metrics
                vix_data = instruments.get('data', {}).get('VIX', {})
                features['vix_level'] = vix_data.get('last_price', 15.0)
                
                dxy_data = instruments.get('data', {}).get('DXY', {})
                features['dxy_level'] = dxy_data.get('last_price', 100.0)
                
                sp500_data = instruments.get('data', {}).get('SP500', {})
                features['sp500_daily_return'] = sp500_data.get('daily_return', 0.0)
                
                btc_data = instruments.get('data', {}).get('BTCUSD', {})
                features['btc_volatility_90d'] = btc_data.get('volatility_90d', 0.5)
            except:
                features.update({
                    'vix_level': 15.0,
                    'dxy_level': 100.0,
                    'sp500_daily_return': 0.0,
                    'btc_volatility_90d': 0.5
                })
            
            # Parse economic indicators
            try:
                indicators = json.loads(regime_data['country_economic_indicators']) if isinstance(
                    regime_data['country_economic_indicators'], str
                ) else regime_data['country_economic_indicators']
                
                features['fed_funds_rate'] = indicators.get('fed_funds_rate_effective', 5.0)
            except:
                features['fed_funds_rate'] = 5.0
        
        else:
            # Default values if no regime data found
            features.update({
                'market_sentiment_score': 0.0,
                'market_volatility_regime': 'normal',
                'market_liquidity_state': 'normal',
                'vix_level': 15.0,
                'dxy_level': 100.0,
                'sp500_daily_return': 0.0,
                'btc_volatility_90d': 0.5,
                'fed_funds_rate': 5.0
            })
        
        return features
    
    def _get_time_features(self, feature_date: date) -> Dict[str, Any]:
        """Calculate date and time features."""
        features = {
            'day_of_week': feature_date.weekday(),  # 0 = Monday, 6 = Sunday
            'week_of_month': (feature_date.day - 1) // 7 + 1,
            'month': feature_date.month,
            'quarter': (feature_date.month - 1) // 3 + 1,
            'day_of_year': feature_date.timetuple().tm_yday,
            'is_month_start': feature_date.day <= 3,
            'is_month_end': feature_date.day >= 28,
            'is_quarter_start': feature_date.month in [1, 4, 7, 10] and feature_date.day <= 3,
            'is_quarter_end': feature_date.month in [3, 6, 9, 12] and feature_date.day >= 28
        }
        
        return features
    
    def _save_features(self, features: Dict[str, Any]):
        """Save calculated features to the database."""
        # Convert feature dict to match database columns
        columns = list(features.keys())
        values = [features[col] for col in columns]
        
        # Build insert query with ON CONFLICT
        placeholders = ', '.join(['%s'] * len(columns))
        columns_str = ', '.join(columns)
        
        query = f"""
        INSERT INTO {self.feature_table} ({columns_str})
        VALUES ({placeholders})
        ON CONFLICT (account_id, feature_date) 
        DO UPDATE SET
            {', '.join([f"{col} = EXCLUDED.{col}" for col in columns if col not in ['account_id', 'feature_date']])}
        """
        
        with self.db_manager.model_db.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(query, values)


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Engineer features for daily profit model')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Start date for feature engineering (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='End date for feature engineering (YYYY-MM-DD)')
    parser.add_argument('--force-rebuild', action='store_true',
                       help='Force rebuild of existing features')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='engineer_features')
    
    # Run feature engineering
    engineer = FeatureEngineer()
    try:
        records = engineer.engineer_features(
            start_date=args.start_date,
            end_date=args.end_date,
            force_rebuild=args.force_rebuild
        )
        logger.info(f"Feature engineering complete. Total records: {records}")
    except Exception as e:
        logger.error(f"Feature engineering failed: {str(e)}")
        raise


if __name__ == '__main__':
    main()

================
File: src/modeling/__init__.py
================
# Model training and prediction modules for the daily profit model

================
File: src/modeling/predict_daily.py
================
"""
Generate daily predictions for all eligible accounts.
Loads the latest data, runs feature engineering, and generates predictions for T+1.
"""

import os
import sys
import logging
from datetime import datetime, timedelta, date
from typing import Dict, List, Any, Optional, Tuple
import argparse
import pandas as pd
import numpy as np
import json
import joblib
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class DailyPredictor:
    """Handles daily prediction generation for all eligible accounts."""
    
    def __init__(self, model_version: Optional[str] = None):
        """
        Initialize the daily predictor.
        
        Args:
            model_version: Version of model to use (defaults to active model)
        """
        self.db_manager = get_db_manager()
        self.model_version = model_version
        
        # Load model artifacts
        self.model = None
        self.scaler = None
        self.feature_columns = None
        self.model_metadata = None
        
        self._load_model()
    
    def _load_model(self):
        """Load the model and associated artifacts."""
        # Get model metadata from registry
        if self.model_version:
            query = """
            SELECT * FROM model_registry 
            WHERE model_version = %s
            """
            params = (self.model_version,)
        else:
            # Get the active model
            query = """
            SELECT * FROM model_registry 
            WHERE is_active = true
            ORDER BY created_at DESC
            LIMIT 1
            """
            params = None
        
        result = self.db_manager.model_db.execute_query(query, params)
        
        if not result:
            raise ValueError(f"No model found with version: {self.model_version}")
        
        self.model_metadata = result[0]
        self.model_version = self.model_metadata['model_version']
        
        logger.info(f"Loading model version: {self.model_version}")
        
        # Load model artifacts
        model_path = Path(self.model_metadata['model_file_path'])
        scaler_path = Path(self.model_metadata['scaler_file_path'])
        
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found: {model_path}")
        if not scaler_path.exists():
            raise FileNotFoundError(f"Scaler file not found: {scaler_path}")
        
        # Load model and scaler
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)
        
        # Load feature columns
        features_path = model_path.parent / 'feature_columns.json'
        with open(features_path, 'r') as f:
            self.feature_columns = json.load(f)
        
        logger.info(f"Model loaded successfully: {self.model_version}")
    
    def predict_daily(self, 
                     prediction_date: Optional[date] = None,
                     save_predictions: bool = True) -> pd.DataFrame:
        """
        Generate predictions for all eligible accounts for a specific date.
        
        Args:
            prediction_date: Date to predict for (T+1). If None, predicts for tomorrow.
            save_predictions: Whether to save predictions to database
            
        Returns:
            DataFrame containing predictions
        """
        start_time = datetime.now()
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='predict_daily',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Determine prediction date (T+1)
            if prediction_date is None:
                prediction_date = datetime.now().date()  # Predict for today based on yesterday's features
            
            feature_date = prediction_date - timedelta(days=1)  # T
            
            logger.info(f"Generating predictions for {prediction_date} using features from {feature_date}")
            
            # Get features for all eligible accounts
            features_df = self._get_features_for_prediction(feature_date)
            
            if features_df.empty:
                logger.warning(f"No features found for date {feature_date}")
                return pd.DataFrame()
            
            logger.info(f"Found features for {len(features_df)} accounts")
            
            # Prepare features for prediction
            X = self._prepare_features(features_df)
            
            # Generate predictions
            predictions_raw = self.model.predict(X)
            
            # Create predictions DataFrame
            predictions_df = pd.DataFrame({
                'login': features_df['login'],
                'prediction_date': prediction_date,
                'feature_date': feature_date,
                'predicted_net_profit': predictions_raw,
                'model_version': self.model_version
            })
            
            # Calculate prediction confidence (based on historical accuracy)
            predictions_df['prediction_confidence'] = self._calculate_confidence(
                features_df, predictions_raw
            )
            
            # Save predictions if requested
            if save_predictions:
                saved_count = self._save_predictions(predictions_df)
                logger.info(f"Saved {saved_count} predictions to database")
            
            # Log summary statistics
            logger.info(f"Prediction summary for {prediction_date}:")
            logger.info(f"  - Total accounts: {len(predictions_df)}")
            logger.info(f"  - Mean predicted PnL: ${predictions_df['predicted_net_profit'].mean():.2f}")
            logger.info(f"  - Std predicted PnL: ${predictions_df['predicted_net_profit'].std():.2f}")
            logger.info(f"  - Predicted winners: {(predictions_df['predicted_net_profit'] > 0).sum()}")
            logger.info(f"  - Predicted losers: {(predictions_df['predicted_net_profit'] < 0).sum()}")
            
            # Identify high-risk accounts (large predicted losses)
            high_risk_threshold = predictions_df['predicted_net_profit'].quantile(0.05)
            high_risk_accounts = predictions_df[
                predictions_df['predicted_net_profit'] < high_risk_threshold
            ]
            logger.info(f"  - High risk accounts (bottom 5%): {len(high_risk_accounts)}")
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='predict_daily',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=len(predictions_df),
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'prediction_date': str(prediction_date),
                    'feature_date': str(feature_date),
                    'model_version': self.model_version,
                    'accounts_predicted': len(predictions_df),
                    'mean_prediction': float(predictions_df['predicted_net_profit'].mean())
                }
            )
            
            return predictions_df
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='predict_daily',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e)
            )
            logger.error(f"Failed to generate predictions: {str(e)}")
            raise
    
    def _get_features_for_prediction(self, feature_date: date) -> pd.DataFrame:
        """Get features for all eligible accounts for the feature date."""
        query = """
        SELECT f.*, s.phase, s.status
        FROM feature_store_account_daily f
        JOIN stg_accounts_daily_snapshots s
            ON f.account_id = s.account_id AND f.feature_date = s.date
        WHERE f.feature_date = %s
            AND s.phase = 'Funded'
            AND s.status IN ('Active', 'Trading')  -- Adjust based on your status values
        """
        
        features_df = self.db_manager.model_db.execute_query_df(query, (feature_date,))
        
        return features_df
    
    def _prepare_features(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """Prepare features for prediction (scaling, column selection, etc.)."""
        # Select only the feature columns used in training
        X = features_df[self.feature_columns].copy()
        
        # Handle categorical features
        categorical_features = ['market_volatility_regime', 'market_liquidity_state']
        for cat_col in categorical_features:
            if cat_col in X.columns:
                X[cat_col] = X[cat_col].astype('category')
        
        # Identify numerical columns for scaling
        numerical_cols = [col for col in X.columns if col not in categorical_features]
        
        # Apply scaling to numerical features
        X_scaled = X.copy()
        X_scaled[numerical_cols] = self.scaler.transform(X[numerical_cols])
        
        return X_scaled
    
    def _calculate_confidence(self, features_df: pd.DataFrame, 
                            predictions: np.ndarray) -> np.ndarray:
        """
        Calculate prediction confidence based on feature quality and historical accuracy.
        
        This is a simplified confidence calculation. In production, you might:
        - Use prediction intervals from the model
        - Calculate based on similar historical predictions
        - Use ensemble disagreement as uncertainty measure
        """
        confidence_scores = np.ones(len(predictions)) * 0.7  # Base confidence
        
        # Adjust confidence based on data quality
        # Lower confidence for accounts with fewer historical data points
        if 'active_trading_days_count' in features_df.columns:
            # More trading days = higher confidence
            trading_days = features_df['active_trading_days_count'].values
            confidence_adjustment = np.clip(trading_days / 100, 0.5, 1.0)
            confidence_scores *= confidence_adjustment
        
        # Lower confidence for extreme predictions
        pred_std = np.std(predictions)
        pred_mean = np.mean(predictions)
        z_scores = np.abs((predictions - pred_mean) / (pred_std + 1e-6))
        extreme_adjustment = 1.0 - np.clip(z_scores / 3, 0, 0.3)
        confidence_scores *= extreme_adjustment
        
        # Convert to percentage
        confidence_scores = np.clip(confidence_scores * 100, 10, 95)
        
        return confidence_scores
    
    def _save_predictions(self, predictions_df: pd.DataFrame) -> int:
        """Save predictions to the database."""
        # Prepare data for insertion
        records = []
        for _, row in predictions_df.iterrows():
            records.append({
                'login': row['login'],
                'prediction_date': row['prediction_date'],
                'feature_date': row['feature_date'],
                'predicted_net_profit': float(row['predicted_net_profit']),
                'prediction_confidence': float(row['prediction_confidence']),
                'model_version': row['model_version'],
                'shap_values': None,  # Would be calculated if needed
                'top_positive_features': None,
                'top_negative_features': None,
                'actual_net_profit': None,  # Filled later when actual is known
                'prediction_error': None
            })
        
        # Insert into database
        saved_count = 0
        if records:
            # Use batch insert
            columns = list(records[0].keys())
            placeholders = ', '.join(['%s'] * len(columns))
            columns_str = ', '.join(columns)
            
            query = f"""
            INSERT INTO model_predictions ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT (login, prediction_date, model_version) 
            DO UPDATE SET
                predicted_net_profit = EXCLUDED.predicted_net_profit,
                prediction_confidence = EXCLUDED.prediction_confidence
            """
            
            with self.db_manager.model_db.get_connection() as conn:
                with conn.cursor() as cursor:
                    for record in records:
                        values = [record[col] for col in columns]
                        cursor.execute(query, values)
                        saved_count += 1
        
        return saved_count
    
    def evaluate_predictions(self, evaluation_date: Optional[date] = None) -> Dict[str, Any]:
        """
        Evaluate predictions against actual outcomes.
        
        Args:
            evaluation_date: Date to evaluate predictions for
            
        Returns:
            Dictionary containing evaluation metrics
        """
        if evaluation_date is None:
            evaluation_date = datetime.now().date() - timedelta(days=1)
        
        logger.info(f"Evaluating predictions for {evaluation_date}")
        
        # Get predictions and actuals
        query = """
        SELECT 
            p.login,
            p.predicted_net_profit,
            p.prediction_confidence,
            m.net_profit as actual_net_profit
        FROM model_predictions p
        JOIN raw_metrics_daily m
            ON p.login = m.login AND p.prediction_date = m.date
        WHERE p.prediction_date = %s
            AND p.model_version = %s
        """
        
        results_df = self.db_manager.model_db.execute_query_df(
            query, (evaluation_date, self.model_version)
        )
        
        if results_df.empty:
            logger.warning(f"No predictions found for evaluation on {evaluation_date}")
            return {}
        
        # Calculate metrics
        y_true = results_df['actual_net_profit']
        y_pred = results_df['predicted_net_profit']
        
        mae = np.mean(np.abs(y_true - y_pred))
        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
        
        # Directional accuracy
        direction_correct = ((y_true > 0) == (y_pred > 0)).mean() * 100
        
        # Update predictions with actual values and errors
        update_query = """
        UPDATE model_predictions
        SET actual_net_profit = %s,
            prediction_error = %s
        WHERE login = %s 
            AND prediction_date = %s 
            AND model_version = %s
        """
        
        with self.db_manager.model_db.get_connection() as conn:
            with conn.cursor() as cursor:
                for _, row in results_df.iterrows():
                    error = row['predicted_net_profit'] - row['actual_net_profit']
                    cursor.execute(update_query, (
                        row['actual_net_profit'],
                        error,
                        row['login'],
                        evaluation_date,
                        self.model_version
                    ))
        
        evaluation_metrics = {
            'evaluation_date': evaluation_date,
            'model_version': self.model_version,
            'accounts_evaluated': len(results_df),
            'mae': mae,
            'rmse': rmse,
            'direction_accuracy': direction_correct,
            'actual_mean': y_true.mean(),
            'predicted_mean': y_pred.mean(),
            'actual_std': y_true.std(),
            'predicted_std': y_pred.std()
        }
        
        logger.info(f"Evaluation complete: MAE=${mae:.2f}, Direction accuracy={direction_correct:.1f}%")
        
        return evaluation_metrics


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Generate daily predictions')
    parser.add_argument('--prediction-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Date to predict for (YYYY-MM-DD). Defaults to today.')
    parser.add_argument('--model-version', help='Model version to use (defaults to active model)')
    parser.add_argument('--no-save', action='store_true',
                       help='Do not save predictions to database')
    parser.add_argument('--evaluate', action='store_true',
                       help='Evaluate yesterday\'s predictions against actuals')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='predict_daily')
    
    # Create predictor
    predictor = DailyPredictor(model_version=args.model_version)
    
    try:
        if args.evaluate:
            # Evaluate previous predictions
            metrics = predictor.evaluate_predictions()
            if metrics:
                logger.info("\nEvaluation Results:")
                logger.info(f"  Date: {metrics['evaluation_date']}")
                logger.info(f"  Accounts: {metrics['accounts_evaluated']}")
                logger.info(f"  MAE: ${metrics['mae']:.2f}")
                logger.info(f"  RMSE: ${metrics['rmse']:.2f}")
                logger.info(f"  Direction Accuracy: {metrics['direction_accuracy']:.1f}%")
        else:
            # Generate predictions
            predictions = predictor.predict_daily(
                prediction_date=args.prediction_date,
                save_predictions=not args.no_save
            )
            
            if not predictions.empty:
                # Display top predictions
                logger.info("\nTop 10 Predicted Profits:")
                top_profits = predictions.nlargest(10, 'predicted_net_profit')
                for _, row in top_profits.iterrows():
                    logger.info(f"  {row['login']}: ${row['predicted_net_profit']:.2f} "
                              f"(confidence: {row['prediction_confidence']:.1f}%)")
                
                logger.info("\nTop 10 Predicted Losses:")
                top_losses = predictions.nsmallest(10, 'predicted_net_profit')
                for _, row in top_losses.iterrows():
                    logger.info(f"  {row['login']}: ${row['predicted_net_profit']:.2f} "
                              f"(confidence: {row['prediction_confidence']:.1f}%)")
    
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        raise


if __name__ == '__main__':
    main()

================
File: src/modeling/train_model.py
================
"""
Train LightGBM model for daily profit prediction.
Implements time-series aware splitting, hyperparameter tuning, and model evaluation.
"""

import os
import sys
import logging
from datetime import datetime, date
from typing import Dict, List, Any, Optional, Tuple
import argparse
import pandas as pd
import numpy as np
import json
import joblib
from pathlib import Path

# Machine learning imports
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import optuna
from optuna.samplers import TPESampler
import shap

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class ModelTrainer:
    """Handles training of the LightGBM model for daily profit prediction."""
    
    def __init__(self, model_version: Optional[str] = None):
        """
        Initialize the model trainer.
        
        Args:
            model_version: Version identifier for the model
        """
        self.db_manager = get_db_manager()
        self.model_version = model_version or f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Model artifacts directory
        self.artifacts_dir = Path("model_artifacts") / self.model_version
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        # Feature columns (exclude target and metadata)
        self.exclude_columns = [
            'id', 'login', 'prediction_date', 'feature_date', 
            'target_net_profit', 'created_at'
        ]
        
        # Categorical features
        self.categorical_features = [
            'market_volatility_regime', 'market_liquidity_state'
        ]
        
        # Model and preprocessing objects
        self.model = None
        self.scaler = None
        self.feature_columns = None
        self.best_params = None
    
    def train_model(self,
                   train_ratio: float = 0.7,
                   val_ratio: float = 0.15,
                   tune_hyperparameters: bool = True,
                   n_trials: int = 50) -> Dict[str, Any]:
        """
        Train the LightGBM model with time-series aware splitting.
        
        Args:
            train_ratio: Proportion of data for training
            val_ratio: Proportion of data for validation
            tune_hyperparameters: Whether to tune hyperparameters
            n_trials: Number of Optuna trials for hyperparameter tuning
            
        Returns:
            Dictionary containing training results and metrics
        """
        start_time = datetime.now()
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='train_model',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            logger.info(f"Starting model training - Version: {self.model_version}")
            
            # Load training data
            logger.info("Loading training data...")
            X, y, dates = self._load_training_data()
            logger.info(f"Loaded {len(X)} training samples")
            
            # Split data (time-series aware)
            logger.info("Splitting data for training, validation, and testing...")
            splits = self._split_data(X, y, dates, train_ratio, val_ratio)
            X_train, X_val, X_test = splits['X_train'], splits['X_val'], splits['X_test']
            y_train, y_val, y_test = splits['y_train'], splits['y_val'], splits['y_test']
            dates_train, dates_val, dates_test = splits['dates_train'], splits['dates_val'], splits['dates_test']
            
            logger.info(f"Train: {len(X_train)} samples ({dates_train.min()} to {dates_train.max()})")
            logger.info(f"Val: {len(X_val)} samples ({dates_val.min()} to {dates_val.max()})")
            logger.info(f"Test: {len(X_test)} samples ({dates_test.min()} to {dates_test.max()})")
            
            # Baseline model
            baseline_metrics = self._evaluate_baseline(y_train, y_val, y_test)
            logger.info(f"Baseline MAE (predict previous day): {baseline_metrics['test_mae']:.2f}")
            
            # Scale features
            logger.info("Scaling features...")
            X_train_scaled, X_val_scaled, X_test_scaled = self._scale_features(
                X_train, X_val, X_test
            )
            
            # Hyperparameter tuning
            if tune_hyperparameters:
                logger.info(f"Starting hyperparameter tuning with {n_trials} trials...")
                self.best_params = self._tune_hyperparameters(
                    X_train_scaled, y_train, X_val_scaled, y_val, n_trials
                )
                logger.info(f"Best parameters: {self.best_params}")
            else:
                # Use default parameters
                self.best_params = self._get_default_params()
            
            # Train final model
            logger.info("Training final model...")
            self.model = self._train_lightgbm(
                X_train_scaled, y_train, X_val_scaled, y_val, self.best_params
            )
            
            # Evaluate model
            logger.info("Evaluating model...")
            metrics = self._evaluate_model(
                X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test
            )
            
            # Calculate feature importance
            logger.info("Calculating feature importance...")
            feature_importance = self._calculate_feature_importance()
            
            # Calculate SHAP values for test set
            logger.info("Calculating SHAP values...")
            shap_values = self._calculate_shap_values(X_test_scaled[:1000])  # Sample for speed
            
            # Save model and artifacts
            logger.info("Saving model artifacts...")
            self._save_artifacts(metrics, feature_importance)
            
            # Register model in database
            self._register_model(
                dates_train, dates_val, dates_test, metrics, feature_importance
            )
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='train_model',
                execution_date=datetime.now().date(),
                status='success',
                execution_details={
                    'model_version': self.model_version,
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'train_samples': len(X_train),
                    'val_samples': len(X_val),
                    'test_samples': len(X_test),
                    'test_mae': metrics['test_mae'],
                    'test_r2': metrics['test_r2']
                }
            )
            
            # Return results
            results = {
                'model_version': self.model_version,
                'metrics': metrics,
                'baseline_metrics': baseline_metrics,
                'feature_importance': feature_importance,
                'best_params': self.best_params,
                'data_splits': {
                    'train': {'start': dates_train.min(), 'end': dates_train.max(), 'samples': len(X_train)},
                    'val': {'start': dates_val.min(), 'end': dates_val.max(), 'samples': len(X_val)},
                    'test': {'start': dates_test.min(), 'end': dates_test.max(), 'samples': len(X_test)}
                }
            }
            
            logger.info(f"Model training completed successfully - Version: {self.model_version}")
            return results
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='train_model',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e)
            )
            logger.error(f"Failed to train model: {str(e)}")
            raise
    
    def _load_training_data(self) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """Load and prepare training data from the database."""
        query = """
        SELECT *
        FROM model_training_input
        ORDER BY prediction_date
        """
        
        df = self.db_manager.model_db.execute_query_df(query)
        
        if df.empty:
            raise ValueError("No training data found in model_training_input table")
        
        # Separate features, target, and dates
        dates = pd.to_datetime(df['prediction_date'])
        y = df['target_net_profit']
        
        # Get feature columns
        self.feature_columns = [col for col in df.columns if col not in self.exclude_columns]
        X = df[self.feature_columns]
        
        # Handle categorical features
        for cat_col in self.categorical_features:
            if cat_col in X.columns:
                X[cat_col] = X[cat_col].astype('category')
        
        logger.info(f"Feature columns: {len(self.feature_columns)}")
        logger.info(f"Date range: {dates.min()} to {dates.max()}")
        logger.info(f"Target statistics - Mean: ${y.mean():.2f}, Std: ${y.std():.2f}")
        
        return X, y, dates
    
    def _split_data(self, X: pd.DataFrame, y: pd.Series, dates: pd.Series,
                   train_ratio: float, val_ratio: float) -> Dict[str, Any]:
        """Split data using time-series aware approach."""
        n_samples = len(X)
        train_size = int(n_samples * train_ratio)
        val_size = int(n_samples * val_ratio)
        
        # Time-based splitting
        X_train = X.iloc[:train_size]
        y_train = y.iloc[:train_size]
        dates_train = dates.iloc[:train_size]
        
        X_val = X.iloc[train_size:train_size + val_size]
        y_val = y.iloc[train_size:train_size + val_size]
        dates_val = dates.iloc[train_size:train_size + val_size]
        
        X_test = X.iloc[train_size + val_size:]
        y_test = y.iloc[train_size + val_size:]
        dates_test = dates.iloc[train_size + val_size:]
        
        return {
            'X_train': X_train, 'y_train': y_train, 'dates_train': dates_train,
            'X_val': X_val, 'y_val': y_val, 'dates_val': dates_val,
            'X_test': X_test, 'y_test': y_test, 'dates_test': dates_test
        }
    
    def _evaluate_baseline(self, y_train: pd.Series, y_val: pd.Series, 
                         y_test: pd.Series) -> Dict[str, float]:
        """Evaluate simple baseline model (predict previous day's PnL)."""
        # For baseline, predict the mean of training data
        train_mean = y_train.mean()
        
        baseline_pred_val = np.full(len(y_val), train_mean)
        baseline_pred_test = np.full(len(y_test), train_mean)
        
        return {
            'val_mae': mean_absolute_error(y_val, baseline_pred_val),
            'val_rmse': np.sqrt(mean_squared_error(y_val, baseline_pred_val)),
            'test_mae': mean_absolute_error(y_test, baseline_pred_test),
            'test_rmse': np.sqrt(mean_squared_error(y_test, baseline_pred_test))
        }
    
    def _scale_features(self, X_train: pd.DataFrame, X_val: pd.DataFrame,
                       X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """Scale numerical features while preserving categorical ones."""
        # Identify numerical columns
        numerical_cols = [col for col in X_train.columns 
                         if col not in self.categorical_features]
        
        # Initialize scaler
        self.scaler = StandardScaler()
        
        # Fit on training data and transform all sets
        X_train_scaled = X_train.copy()
        X_val_scaled = X_val.copy()
        X_test_scaled = X_test.copy()
        
        X_train_scaled[numerical_cols] = self.scaler.fit_transform(X_train[numerical_cols])
        X_val_scaled[numerical_cols] = self.scaler.transform(X_val[numerical_cols])
        X_test_scaled[numerical_cols] = self.scaler.transform(X_test[numerical_cols])
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def _get_default_params(self) -> Dict[str, Any]:
        """Get default LightGBM parameters."""
        return {
            'objective': 'regression_l1',  # MAE
            'metric': 'mae',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 20,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'n_estimators': 300,
            'random_state': 42,
            'verbosity': -1
        }
    
    def _tune_hyperparameters(self, X_train: pd.DataFrame, y_train: pd.Series,
                            X_val: pd.DataFrame, y_val: pd.Series,
                            n_trials: int) -> Dict[str, Any]:
        """Tune hyperparameters using Optuna with time series cross-validation."""
        
        def objective(trial):
            params = {
                'objective': trial.suggest_categorical('objective', ['regression_l1', 'huber', 'fair']),
                'metric': 'mae',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 10, 100),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0, log=True),
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'random_state': 42,
                'verbosity': -1
            }
            
            # Additional parameters for robust objectives
            if params['objective'] == 'huber':
                params['alpha'] = trial.suggest_float('huber_alpha', 0.5, 2.0)
            elif params['objective'] == 'fair':
                params['fair_c'] = trial.suggest_float('fair_c', 0.5, 2.0)
            
            # Train model with current parameters
            model = lgb.LGBMRegressor(**params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)],
                categorical_feature=self.categorical_features
            )
            
            # Evaluate on validation set
            y_pred = model.predict(X_val)
            mae = mean_absolute_error(y_val, y_pred)
            
            return mae
        
        # Create Optuna study
        study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))
        study.optimize(objective, n_trials=n_trials)
        
        logger.info(f"Best trial MAE: {study.best_value:.4f}")
        
        return study.best_params
    
    def _train_lightgbm(self, X_train: pd.DataFrame, y_train: pd.Series,
                       X_val: pd.DataFrame, y_val: pd.Series,
                       params: Dict[str, Any]) -> lgb.LGBMRegressor:
        """Train the final LightGBM model with best parameters."""
        model = lgb.LGBMRegressor(**params)
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(20)],
            categorical_feature=self.categorical_features
        )
        
        return model
    
    def _evaluate_model(self, X_train: pd.DataFrame, y_train: pd.Series,
                       X_val: pd.DataFrame, y_val: pd.Series,
                       X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:
        """Evaluate model on all data splits."""
        metrics = {}
        
        for name, X, y in [('train', X_train, y_train), 
                          ('val', X_val, y_val), 
                          ('test', X_test, y_test)]:
            y_pred = self.model.predict(X)
            
            metrics[f'{name}_mae'] = mean_absolute_error(y, y_pred)
            metrics[f'{name}_rmse'] = np.sqrt(mean_squared_error(y, y_pred))
            metrics[f'{name}_r2'] = r2_score(y, y_pred)
            
            # Calculate percentiles of absolute errors
            abs_errors = np.abs(y - y_pred)
            metrics[f'{name}_mae_p50'] = np.percentile(abs_errors, 50)
            metrics[f'{name}_mae_p90'] = np.percentile(abs_errors, 90)
            metrics[f'{name}_mae_p95'] = np.percentile(abs_errors, 95)
        
        return metrics
    
    def _calculate_feature_importance(self) -> Dict[str, float]:
        """Calculate and return feature importance scores."""
        importance = self.model.feature_importances_
        feature_importance = dict(zip(self.feature_columns, importance))
        
        # Sort by importance
        feature_importance = dict(sorted(
            feature_importance.items(), 
            key=lambda x: x[1], 
            reverse=True
        ))
        
        # Log top features
        logger.info("Top 10 most important features:")
        for i, (feature, score) in enumerate(list(feature_importance.items())[:10]):
            logger.info(f"  {i+1}. {feature}: {score:.4f}")
        
        return feature_importance
    
    def _calculate_shap_values(self, X_sample: pd.DataFrame) -> np.ndarray:
        """Calculate SHAP values for model interpretability."""
        explainer = shap.TreeExplainer(self.model)
        shap_values = explainer.shap_values(X_sample)
        
        # Save SHAP summary plot
        shap.summary_plot(
            shap_values, X_sample, 
            feature_names=self.feature_columns,
            show=False
        )
        import matplotlib.pyplot as plt
        plt.savefig(self.artifacts_dir / 'shap_summary.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        return shap_values
    
    def _save_artifacts(self, metrics: Dict[str, float], 
                       feature_importance: Dict[str, float]):
        """Save model artifacts to disk."""
        # Save model
        model_path = self.artifacts_dir / 'model.pkl'
        joblib.dump(self.model, model_path)
        logger.info(f"Model saved to {model_path}")
        
        # Save scaler
        scaler_path = self.artifacts_dir / 'scaler.pkl'
        joblib.dump(self.scaler, scaler_path)
        logger.info(f"Scaler saved to {scaler_path}")
        
        # Save feature columns
        features_path = self.artifacts_dir / 'feature_columns.json'
        with open(features_path, 'w') as f:
            json.dump(self.feature_columns, f, indent=2)
        
        # Save hyperparameters
        params_path = self.artifacts_dir / 'hyperparameters.json'
        with open(params_path, 'w') as f:
            json.dump(self.best_params, f, indent=2)
        
        # Save metrics
        metrics_path = self.artifacts_dir / 'metrics.json'
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        # Save feature importance
        importance_path = self.artifacts_dir / 'feature_importance.json'
        with open(importance_path, 'w') as f:
            json.dump(feature_importance, f, indent=2)
    
    def _register_model(self, dates_train: pd.Series, dates_val: pd.Series,
                       dates_test: pd.Series, metrics: Dict[str, float],
                       feature_importance: Dict[str, float]):
        """Register model in the database."""
        query = """
        INSERT INTO model_registry (
            model_version, model_type,
            training_start_date, training_end_date,
            validation_start_date, validation_end_date,
            test_start_date, test_end_date,
            train_mae, train_rmse, train_r2,
            val_mae, val_rmse, val_r2,
            test_mae, test_rmse, test_r2,
            hyperparameters, feature_list, feature_importance,
            model_file_path, scaler_file_path,
            is_active
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        params = (
            self.model_version,
            'LightGBM',
            dates_train.min().date(),
            dates_train.max().date(),
            dates_val.min().date(),
            dates_val.max().date(),
            dates_test.min().date(),
            dates_test.max().date(),
            metrics['train_mae'],
            metrics['train_rmse'],
            metrics['train_r2'],
            metrics['val_mae'],
            metrics['val_rmse'],
            metrics['val_r2'],
            metrics['test_mae'],
            metrics['test_rmse'],
            metrics['test_r2'],
            json.dumps(self.best_params),
            json.dumps(self.feature_columns),
            json.dumps(feature_importance),
            str(self.artifacts_dir / 'model.pkl'),
            str(self.artifacts_dir / 'scaler.pkl'),
            False  # Not active by default
        )
        
        self.db_manager.model_db.execute_command(query, params)
        logger.info(f"Model registered in database: {self.model_version}")


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Train LightGBM model for daily profit prediction')
    parser.add_argument('--model-version', help='Model version identifier')
    parser.add_argument('--train-ratio', type=float, default=0.7,
                       help='Proportion of data for training')
    parser.add_argument('--val-ratio', type=float, default=0.15,
                       help='Proportion of data for validation')
    parser.add_argument('--tune-hyperparameters', action='store_true',
                       help='Perform hyperparameter tuning')
    parser.add_argument('--n-trials', type=int, default=50,
                       help='Number of Optuna trials for tuning')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='train_model')
    
    # Train model
    trainer = ModelTrainer(model_version=args.model_version)
    try:
        results = trainer.train_model(
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            tune_hyperparameters=args.tune_hyperparameters,
            n_trials=args.n_trials
        )
        
        # Print summary
        logger.info("\n" + "="*50)
        logger.info("MODEL TRAINING SUMMARY")
        logger.info("="*50)
        logger.info(f"Model Version: {results['model_version']}")
        logger.info(f"Test MAE: ${results['metrics']['test_mae']:.2f}")
        logger.info(f"Test RMSE: ${results['metrics']['test_rmse']:.2f}")
        logger.info(f"Test R²: {results['metrics']['test_r2']:.4f}")
        logger.info(f"Baseline Test MAE: ${results['baseline_metrics']['test_mae']:.2f}")
        improvement = (
            (results['baseline_metrics']['test_mae'] - results['metrics']['test_mae']) / 
            results['baseline_metrics']['test_mae'] * 100
        )
        logger.info(f"Improvement over baseline: {improvement:.1f}%")
        
    except Exception as e:
        logger.error(f"Model training failed: {str(e)}")
        raise


if __name__ == '__main__':
    main()

================
File: src/pipeline_orchestration/__init__.py
================
# Pipeline orchestration modules for the daily profit model

================
File: src/pipeline_orchestration/run_pipeline.py
================
"""
Main orchestration script for the daily profit model pipeline.
Coordinates execution of all pipeline stages from data ingestion to predictions.
"""

import os
import sys
import logging
import argparse
from datetime import datetime, timedelta, date
from typing import Dict, List, Any, Optional
import subprocess
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager, close_db_connections
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class PipelineOrchestrator:
    """Orchestrates the execution of the daily profit model pipeline."""
    
    def __init__(self):
        """Initialize the pipeline orchestrator."""
        self.db_manager = get_db_manager()
        self.src_dir = Path(__file__).parent.parent
        
        # Pipeline stages configuration
        self.stages = {
            'schema': {
                'name': 'Database Schema Creation',
                'script': None,  # Special handling
                'module': None
            },
            'ingestion': {
                'name': 'Data Ingestion',
                'scripts': [
                    ('ingest_accounts', 'data_ingestion.ingest_accounts'),
                    ('ingest_plans', 'data_ingestion.ingest_plans'),
                    ('ingest_regimes', 'data_ingestion.ingest_regimes'),
                    ('ingest_metrics_alltime', 'data_ingestion.ingest_metrics'),
                    ('ingest_metrics_daily', 'data_ingestion.ingest_metrics'),
                    ('ingest_trades_open', 'data_ingestion.ingest_trades'),
                    ('ingest_trades_closed', 'data_ingestion.ingest_trades')
                ]
            },
            'preprocessing': {
                'name': 'Data Preprocessing',
                'scripts': [
                    ('create_staging_snapshots', 'preprocessing.create_staging_snapshots')
                ]
            },
            'feature_engineering': {
                'name': 'Feature Engineering',
                'scripts': [
                    ('engineer_features', 'feature_engineering.engineer_features'),
                    ('build_training_data', 'feature_engineering.build_training_data')
                ]
            },
            'training': {
                'name': 'Model Training',
                'scripts': [
                    ('train_model', 'modeling.train_model')
                ]
            },
            'prediction': {
                'name': 'Daily Prediction',
                'scripts': [
                    ('predict_daily', 'modeling.predict_daily')
                ]
            }
        }
    
    def run_pipeline(self,
                    stages: Optional[List[str]] = None,
                    start_date: Optional[date] = None,
                    end_date: Optional[date] = None,
                    skip_completed: bool = True,
                    dry_run: bool = False) -> Dict[str, Any]:
        """
        Run the pipeline stages.
        
        Args:
            stages: List of stages to run. If None, runs all stages.
            start_date: Start date for data processing
            end_date: End date for data processing
            skip_completed: Skip stages that have already completed successfully today
            dry_run: If True, only show what would be executed
            
        Returns:
            Dictionary containing execution results
        """
        start_time = datetime.now()
        results = {}
        
        # Determine stages to run
        if stages is None:
            stages = list(self.stages.keys())
        
        # Validate stages
        invalid_stages = set(stages) - set(self.stages.keys())
        if invalid_stages:
            raise ValueError(f"Invalid stages: {invalid_stages}")
        
        logger.info(f"Pipeline execution started at {start_time}")
        logger.info(f"Stages to run: {stages}")
        
        if dry_run:
            logger.info("DRY RUN MODE - No actual execution")
        
        # Execute each stage
        for stage_name in stages:
            stage_start = datetime.now()
            
            logger.info(f"\n{'='*60}")
            logger.info(f"Stage: {self.stages[stage_name]['name']}")
            logger.info(f"{'='*60}")
            
            try:
                if stage_name == 'schema':
                    # Special handling for schema creation
                    if not dry_run:
                        self._create_schema()
                    results[stage_name] = {'status': 'success', 'duration': 0}
                
                elif stage_name == 'ingestion':
                    results[stage_name] = self._run_ingestion(
                        start_date, end_date, skip_completed, dry_run
                    )
                
                elif stage_name == 'preprocessing':
                    results[stage_name] = self._run_preprocessing(
                        start_date, end_date, skip_completed, dry_run
                    )
                
                elif stage_name == 'feature_engineering':
                    results[stage_name] = self._run_feature_engineering(
                        start_date, end_date, skip_completed, dry_run
                    )
                
                elif stage_name == 'training':
                    results[stage_name] = self._run_training(dry_run)
                
                elif stage_name == 'prediction':
                    results[stage_name] = self._run_prediction(dry_run)
                
                stage_duration = (datetime.now() - stage_start).total_seconds()
                results[stage_name]['duration'] = stage_duration
                logger.info(f"Stage completed in {stage_duration:.2f} seconds")
                
            except Exception as e:
                logger.error(f"Stage {stage_name} failed: {str(e)}")
                results[stage_name] = {
                    'status': 'failed',
                    'error': str(e),
                    'duration': (datetime.now() - stage_start).total_seconds()
                }
                
                # Stop pipeline on failure
                break
        
        # Summary
        total_duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"\n{'='*60}")
        logger.info("PIPELINE EXECUTION SUMMARY")
        logger.info(f"{'='*60}")
        logger.info(f"Total duration: {total_duration:.2f} seconds")
        
        for stage_name, result in results.items():
            status = result.get('status', 'unknown')
            duration = result.get('duration', 0)
            logger.info(f"{stage_name}: {status} ({duration:.2f}s)")
        
        return results
    
    def _create_schema(self):
        """Create the database schema."""
        schema_file = self.src_dir / 'db_schema' / 'schema.sql'
        
        if not schema_file.exists():
            raise FileNotFoundError(f"Schema file not found: {schema_file}")
        
        logger.info(f"Creating database schema from {schema_file}")
        
        # Read schema file
        with open(schema_file, 'r') as f:
            schema_sql = f.read()
        
        # Execute schema creation
        with self.db_manager.model_db.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(schema_sql)
        
        logger.info("Database schema created successfully")
    
    def _run_ingestion(self, start_date: Optional[date], end_date: Optional[date],
                      skip_completed: bool, dry_run: bool) -> Dict[str, Any]:
        """Run data ingestion scripts."""
        results = {'status': 'success', 'scripts': {}}
        
        # Ingestion commands
        commands = [
            # Accounts - full refresh
            ('ingest_accounts', ['--log-level', 'INFO']),
            
            # Plans - from CSV
            ('ingest_plans', ['--log-level', 'INFO']),
            
            # Regimes - with date range
            ('ingest_regimes', [
                '--start-date', str(start_date) if start_date else '2024-01-01',
                '--end-date', str(end_date) if end_date else str(date.today() - timedelta(days=1)),
                '--log-level', 'INFO'
            ]),
            
            # Metrics alltime
            ('ingest_metrics_alltime', ['alltime', '--log-level', 'INFO']),
            
            # Metrics daily
            ('ingest_metrics_daily', [
                'daily',
                '--start-date', str(start_date) if start_date else '2024-01-01',
                '--end-date', str(end_date) if end_date else str(date.today() - timedelta(days=1)),
                '--log-level', 'INFO'
            ]),
            
            # Trades open - only recent
            ('ingest_trades_open', [
                'open',
                '--end-date', str(end_date) if end_date else str(date.today() - timedelta(days=1)),
                '--log-level', 'INFO'
            ]),
            
            # Trades closed - with date range and batching
            ('ingest_trades_closed', [
                'closed',
                '--start-date', str(start_date) if start_date else '2024-01-01',
                '--end-date', str(end_date) if end_date else str(date.today() - timedelta(days=1)),
                '--batch-days', '7',
                '--log-level', 'INFO'
            ])
        ]
        
        for script_name, args in commands:
            if skip_completed and self._is_stage_completed(script_name):
                logger.info(f"Skipping {script_name} - already completed today")
                results['scripts'][script_name] = 'skipped'
                continue
            
            if dry_run:
                logger.info(f"Would run: python -m data_ingestion.{script_name} {' '.join(args)}")
                results['scripts'][script_name] = 'dry_run'
            else:
                success = self._run_python_module(f"data_ingestion.{script_name}", args)
                results['scripts'][script_name] = 'success' if success else 'failed'
                if not success:
                    results['status'] = 'failed'
                    break
        
        return results
    
    def _run_preprocessing(self, start_date: Optional[date], end_date: Optional[date],
                         skip_completed: bool, dry_run: bool) -> Dict[str, Any]:
        """Run preprocessing scripts."""
        results = {'status': 'success', 'scripts': {}}
        
        # Preprocessing commands
        commands = [
            ('create_staging_snapshots', [
                '--start-date', str(start_date) if start_date else '2024-01-01',
                '--end-date', str(end_date) if end_date else str(date.today() - timedelta(days=1)),
                '--clean-data',
                '--log-level', 'INFO'
            ])
        ]
        
        for script_name, args in commands:
            if skip_completed and self._is_stage_completed(script_name):
                logger.info(f"Skipping {script_name} - already completed today")
                results['scripts'][script_name] = 'skipped'
                continue
            
            if dry_run:
                logger.info(f"Would run: python -m preprocessing.{script_name} {' '.join(args)}")
                results['scripts'][script_name] = 'dry_run'
            else:
                success = self._run_python_module(f"preprocessing.{script_name}", args)
                results['scripts'][script_name] = 'success' if success else 'failed'
                if not success:
                    results['status'] = 'failed'
        
        return results
    
    def _run_feature_engineering(self, start_date: Optional[date], end_date: Optional[date],
                               skip_completed: bool, dry_run: bool) -> Dict[str, Any]:
        """Run feature engineering scripts."""
        results = {'status': 'success', 'scripts': {}}
        
        # Feature engineering commands
        commands = [
            ('engineer_features', [
                '--start-date', str(start_date) if start_date else '2024-01-01',
                '--end-date', str(end_date) if end_date else str(date.today() - timedelta(days=1)),
                '--log-level', 'INFO'
            ]),
            ('build_training_data', [
                '--start-date', str(start_date) if start_date else '2024-01-01',
                '--end-date', str(end_date) if end_date else str(date.today() - timedelta(days=2)),
                '--validate',
                '--log-level', 'INFO'
            ])
        ]
        
        for script_name, args in commands:
            if skip_completed and self._is_stage_completed(script_name):
                logger.info(f"Skipping {script_name} - already completed today")
                results['scripts'][script_name] = 'skipped'
                continue
            
            if dry_run:
                logger.info(f"Would run: python -m feature_engineering.{script_name} {' '.join(args)}")
                results['scripts'][script_name] = 'dry_run'
            else:
                success = self._run_python_module(f"feature_engineering.{script_name}", args)
                results['scripts'][script_name] = 'success' if success else 'failed'
                if not success:
                    results['status'] = 'failed'
        
        return results
    
    def _run_training(self, dry_run: bool) -> Dict[str, Any]:
        """Run model training."""
        results = {'status': 'success', 'scripts': {}}
        
        # Training command
        args = [
            '--tune-hyperparameters',
            '--n-trials', '50',
            '--log-level', 'INFO'
        ]
        
        if dry_run:
            logger.info(f"Would run: python -m modeling.train_model {' '.join(args)}")
            results['scripts']['train_model'] = 'dry_run'
        else:
            success = self._run_python_module('modeling.train_model', args)
            results['scripts']['train_model'] = 'success' if success else 'failed'
            if not success:
                results['status'] = 'failed'
        
        return results
    
    def _run_prediction(self, dry_run: bool) -> Dict[str, Any]:
        """Run daily predictions."""
        results = {'status': 'success', 'scripts': {}}
        
        # Prediction command - predict for today based on yesterday's features
        args = ['--log-level', 'INFO']
        
        if dry_run:
            logger.info(f"Would run: python -m modeling.predict_daily {' '.join(args)}")
            results['scripts']['predict_daily'] = 'dry_run'
        else:
            success = self._run_python_module('modeling.predict_daily', args)
            results['scripts']['predict_daily'] = 'success' if success else 'failed'
            if not success:
                results['status'] = 'failed'
        
        # Also run evaluation of previous predictions
        eval_args = ['--evaluate', '--log-level', 'INFO']
        if dry_run:
            logger.info(f"Would run: python -m modeling.predict_daily {' '.join(eval_args)}")
            results['scripts']['evaluate_predictions'] = 'dry_run'
        else:
            success = self._run_python_module('modeling.predict_daily', eval_args)
            results['scripts']['evaluate_predictions'] = 'success' if success else 'failed'
        
        return results
    
    def _run_python_module(self, module: str, args: List[str]) -> bool:
        """
        Run a Python module as a subprocess.
        
        Args:
            module: Module to run (e.g., 'data_ingestion.ingest_accounts')
            args: Command line arguments
            
        Returns:
            True if successful, False otherwise
        """
        try:
            cmd = [sys.executable, '-m', module] + args
            logger.info(f"Running: {' '.join(cmd)}")
            
            # Run with environment variables
            env = os.environ.copy()
            
            result = subprocess.run(
                cmd,
                cwd=self.src_dir,
                env=env,
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                logger.info(f"Successfully executed {module}")
                return True
            else:
                logger.error(f"Failed to execute {module}")
                logger.error(f"Error output: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"Exception running {module}: {str(e)}")
            return False
    
    def _is_stage_completed(self, stage_name: str) -> bool:
        """Check if a stage has completed successfully today."""
        query = """
        SELECT COUNT(*) as count
        FROM pipeline_execution_log
        WHERE pipeline_stage = %s
            AND execution_date = %s
            AND status = 'success'
        """
        
        result = self.db_manager.model_db.execute_query(
            query, (stage_name, date.today())
        )
        
        return result[0]['count'] > 0 if result else False


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(
        description='Run the daily profit model pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Available stages:
  schema              - Create database schema
  ingestion          - Ingest data from APIs and CSV files
  preprocessing      - Create staging snapshots and clean data
  feature_engineering - Engineer features and build training data
  training           - Train the LightGBM model
  prediction         - Generate daily predictions

Examples:
  # Run the entire pipeline
  python run_pipeline.py
  
  # Run only ingestion and preprocessing
  python run_pipeline.py --stages ingestion preprocessing
  
  # Run daily prediction only
  python run_pipeline.py --stages prediction
  
  # Dry run to see what would be executed
  python run_pipeline.py --dry-run
        """
    )
    
    parser.add_argument('--stages', nargs='+', 
                       choices=['schema', 'ingestion', 'preprocessing', 
                               'feature_engineering', 'training', 'prediction'],
                       help='Specific stages to run (default: all stages)')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Start date for data processing (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='End date for data processing (YYYY-MM-DD)')
    parser.add_argument('--force', action='store_true',
                       help='Force re-run of completed stages')
    parser.add_argument('--dry-run', action='store_true',
                       help='Show what would be executed without running')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='pipeline_orchestration')
    
    # Run pipeline
    orchestrator = PipelineOrchestrator()
    try:
        results = orchestrator.run_pipeline(
            stages=args.stages,
            start_date=args.start_date,
            end_date=args.end_date,
            skip_completed=not args.force,
            dry_run=args.dry_run
        )
        
        # Exit with appropriate code
        failed_stages = [s for s, r in results.items() if r.get('status') == 'failed']
        if failed_stages:
            logger.error(f"Pipeline failed. Failed stages: {failed_stages}")
            sys.exit(1)
        else:
            logger.info("Pipeline completed successfully")
            
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}")
        raise
    finally:
        # Clean up database connections
        close_db_connections()


if __name__ == '__main__':
    main()

================
File: src/preprocessing/__init__.py
================
# Data preprocessing modules for the daily profit model

================
File: src/preprocessing/create_staging_snapshots.py
================
"""
Create staging accounts daily snapshots by combining accounts and metrics data.
Creates the stg_accounts_daily_snapshots table for feature engineering.
"""

import os
import sys
import logging
from datetime import datetime, timedelta, date
from typing import Optional
import argparse
import pandas as pd
import numpy as np

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.database import get_db_manager
from utils.logging_config import setup_logging

logger = logging.getLogger(__name__)


class StagingSnapshotCreator:
    """Creates daily account snapshots in the staging layer."""
    
    def __init__(self):
        """Initialize the staging snapshot creator."""
        self.db_manager = get_db_manager()
        self.staging_table = 'stg_accounts_daily_snapshots'
        
    def create_snapshots(self,
                        start_date: Optional[date] = None,
                        end_date: Optional[date] = None,
                        force_rebuild: bool = False) -> int:
        """
        Create daily account snapshots combining accounts and metrics data.
        
        Args:
            start_date: Start date for snapshot creation
            end_date: End date for snapshot creation
            force_rebuild: If True, rebuild snapshots even if they exist
            
        Returns:
            Number of snapshot records created
        """
        start_time = datetime.now()
        total_records = 0
        
        try:
            # Log pipeline execution start
            self.db_manager.log_pipeline_execution(
                pipeline_stage='create_staging_snapshots',
                execution_date=datetime.now().date(),
                status='running'
            )
            
            # Determine date range
            if not end_date:
                end_date = datetime.now().date() - timedelta(days=1)
            if not start_date:
                # Default to last 30 days
                start_date = end_date - timedelta(days=30)
            
            logger.info(f"Creating snapshots from {start_date} to {end_date}")
            
            # Process date by date
            current_date = start_date
            while current_date <= end_date:
                # Check if snapshots already exist for this date
                if not force_rebuild and self._snapshots_exist_for_date(current_date):
                    logger.info(f"Snapshots already exist for {current_date}, skipping...")
                    current_date += timedelta(days=1)
                    continue
                
                # Create snapshots for this date
                date_records = self._create_snapshots_for_date(current_date)
                total_records += date_records
                logger.info(f"Created {date_records} snapshots for {current_date}")
                
                current_date += timedelta(days=1)
            
            # Log successful completion
            self.db_manager.log_pipeline_execution(
                pipeline_stage='create_staging_snapshots',
                execution_date=datetime.now().date(),
                status='success',
                records_processed=total_records,
                execution_details={
                    'duration_seconds': (datetime.now() - start_time).total_seconds(),
                    'start_date': str(start_date),
                    'end_date': str(end_date),
                    'force_rebuild': force_rebuild
                }
            )
            
            logger.info(f"Successfully created {total_records} snapshot records")
            return total_records
            
        except Exception as e:
            # Log failure
            self.db_manager.log_pipeline_execution(
                pipeline_stage='create_staging_snapshots',
                execution_date=datetime.now().date(),
                status='failed',
                error_message=str(e),
                records_processed=total_records
            )
            logger.error(f"Failed to create snapshots: {str(e)}")
            raise
    
    def _snapshots_exist_for_date(self, check_date: date) -> bool:
        """Check if snapshots already exist for a given date."""
        query = f"""
        SELECT COUNT(*) as count 
        FROM {self.staging_table}
        WHERE date = %s
        """
        result = self.db_manager.model_db.execute_query(query, (check_date,))
        return result[0]['count'] > 0
    
    def _create_snapshots_for_date(self, snapshot_date: date) -> int:
        """
        Create snapshots for all eligible accounts on a specific date.
        
        Combines:
        - Latest account state from raw_accounts_data
        - Daily metrics from raw_metrics_daily
        - Plan information from raw_plans_data
        """
        # Delete existing snapshots for this date if any
        delete_query = f"DELETE FROM {self.staging_table} WHERE date = %s"
        self.db_manager.model_db.execute_command(delete_query, (snapshot_date,))
        
        # Query to create snapshots
        # This query gets the latest account state and joins with metrics and plans
        insert_query = f"""
        INSERT INTO {self.staging_table} (
            account_id, login, date, trader_id, plan_id, phase, status,
            starting_balance, current_balance, current_equity,
            profit_target_pct, max_daily_drawdown_pct, max_drawdown_pct,
            max_leverage, is_drawdown_relative,
            days_since_first_trade, active_trading_days_count,
            distance_to_profit_target, distance_to_max_drawdown
        )
        WITH latest_accounts AS (
            -- Get the latest account record for each account as of snapshot_date
            SELECT DISTINCT ON (account_id) 
                account_id, login, trader_id, plan_id, phase, status,
                starting_balance, current_balance, current_equity,
                profit_target_pct, max_daily_drawdown_pct, max_drawdown_pct,
                max_leverage, is_drawdown_relative, breached, is_upgraded
            FROM raw_accounts_data
            WHERE ingestion_timestamp <= %s::timestamp + interval '1 day'
            ORDER BY account_id, ingestion_timestamp DESC
        ),
        account_metrics AS (
            -- Get metrics for the snapshot date
            SELECT 
                account_id,
                balance_end as day_end_balance,
                equity_end as day_end_equity
            FROM raw_metrics_daily
            WHERE date = %s
        ),
        account_history AS (
            -- Calculate days since first trade and active trading days
            SELECT 
                account_id,
                MIN(date) as first_trade_date,
                COUNT(DISTINCT date) as active_days,
                COUNT(DISTINCT CASE WHEN date <= %s THEN date END) as active_days_to_date
            FROM raw_metrics_daily
            WHERE total_trades > 0
            GROUP BY account_id
        ),
        plan_info AS (
            -- Get latest plan information
            SELECT DISTINCT ON (plan_id)
                plan_id,
                profit_target,
                max_drawdown
            FROM raw_plans_data
            ORDER BY plan_id, ingestion_timestamp DESC
        )
        SELECT 
            la.account_id,
            la.login,
            %s::date as date,
            la.trader_id,
            la.plan_id,
            la.phase,
            la.status,
            la.starting_balance,
            COALESCE(am.day_end_balance, la.current_balance) as current_balance,
            COALESCE(am.day_end_equity, la.current_equity) as current_equity,
            la.profit_target_pct,
            la.max_daily_drawdown_pct,
            la.max_drawdown_pct,
            la.max_leverage,
            la.is_drawdown_relative,
            CASE 
                WHEN ah.first_trade_date IS NULL THEN 0
                ELSE %s::date - ah.first_trade_date
            END as days_since_first_trade,
            COALESCE(ah.active_days_to_date, 0) as active_trading_days_count,
            -- Distance to profit target
            CASE 
                WHEN pi.profit_target IS NOT NULL THEN 
                    pi.profit_target - COALESCE(am.day_end_equity, la.current_equity)
                ELSE 
                    la.starting_balance * (la.profit_target_pct / 100.0) - 
                    COALESCE(am.day_end_equity, la.current_equity)
            END as distance_to_profit_target,
            -- Distance to max drawdown (simplified - would need more complex calculation in production)
            CASE
                WHEN la.is_drawdown_relative THEN
                    COALESCE(am.day_end_equity, la.current_equity) - 
                    (la.starting_balance * (1 - la.max_drawdown_pct / 100.0))
                ELSE
                    COALESCE(am.day_end_equity, la.current_equity) - 
                    (la.starting_balance - la.starting_balance * (la.max_drawdown_pct / 100.0))
            END as distance_to_max_drawdown
        FROM latest_accounts la
        LEFT JOIN account_metrics am ON la.account_id = am.account_id
        LEFT JOIN account_history ah ON la.account_id = ah.account_id
        LEFT JOIN plan_info pi ON la.plan_id = pi.plan_id
        WHERE 
            -- Filter for active, funded accounts only
            la.breached = 0
            AND la.is_upgraded = 0
            AND la.phase = 'Funded'
        """
        
        # Execute the insert
        params = (snapshot_date, snapshot_date, snapshot_date, snapshot_date, snapshot_date)
        rows_affected = self.db_manager.model_db.execute_command(insert_query, params)
        
        return rows_affected
    
    def clean_data(self,
                  handle_missing: bool = True,
                  detect_outliers: bool = True) -> None:
        """
        Clean the staging data by handling missing values and outliers.
        
        Args:
            handle_missing: Whether to handle missing values
            detect_outliers: Whether to detect and log outliers
        """
        logger.info("Starting data cleaning process...")
        
        if handle_missing:
            self._handle_missing_values()
        
        if detect_outliers:
            self._detect_outliers()
        
        logger.info("Data cleaning completed")
    
    def _handle_missing_values(self):
        """Handle missing values in the staging table."""
        # Get data profile
        query = f"""
        SELECT 
            COUNT(*) as total_rows,
            COUNT(current_balance) as non_null_balance,
            COUNT(current_equity) as non_null_equity,
            COUNT(days_since_first_trade) as non_null_days,
            COUNT(distance_to_profit_target) as non_null_dist_profit,
            COUNT(distance_to_max_drawdown) as non_null_dist_dd
        FROM {self.staging_table}
        """
        
        result = self.db_manager.model_db.execute_query(query)
        if result:
            profile = result[0]
            total = profile['total_rows']
            
            logger.info(f"Missing value profile for {total} rows:")
            logger.info(f"  - current_balance: {total - profile['non_null_balance']} missing")
            logger.info(f"  - current_equity: {total - profile['non_null_equity']} missing")
            logger.info(f"  - days_since_first_trade: {total - profile['non_null_days']} missing")
        
        # Update missing numerical values with appropriate defaults
        updates = [
            ("UPDATE {} SET days_since_first_trade = 0 WHERE days_since_first_trade IS NULL", 
             "days_since_first_trade"),
            ("UPDATE {} SET active_trading_days_count = 0 WHERE active_trading_days_count IS NULL",
             "active_trading_days_count")
        ]
        
        for update_query, field in updates:
            rows = self.db_manager.model_db.execute_command(update_query.format(self.staging_table))
            if rows > 0:
                logger.info(f"Updated {rows} NULL values for {field}")
    
    def _detect_outliers(self):
        """Detect and log outliers in key fields."""
        # Query to get statistics for outlier detection
        query = f"""
        SELECT 
            PERCENTILE_CONT(0.01) WITHIN GROUP (ORDER BY current_balance) as balance_p1,
            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY current_balance) as balance_q1,
            PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY current_balance) as balance_median,
            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY current_balance) as balance_q3,
            PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY current_balance) as balance_p99,
            AVG(current_balance) as balance_mean,
            STDDEV(current_balance) as balance_std
        FROM {self.staging_table}
        WHERE current_balance IS NOT NULL
        """
        
        result = self.db_manager.model_db.execute_query(query)
        if result:
            stats = result[0]
            
            # Calculate IQR for outlier detection
            iqr = stats['balance_q3'] - stats['balance_q1']
            lower_bound = stats['balance_q1'] - 1.5 * iqr
            upper_bound = stats['balance_q3'] + 1.5 * iqr
            
            logger.info("Balance statistics:")
            logger.info(f"  - Mean: ${stats['balance_mean']:,.2f}")
            logger.info(f"  - Median: ${stats['balance_median']:,.2f}")
            logger.info(f"  - Std Dev: ${stats['balance_std']:,.2f}")
            logger.info(f"  - IQR bounds: ${lower_bound:,.2f} to ${upper_bound:,.2f}")
            
            # Count outliers
            outlier_query = f"""
            SELECT COUNT(*) as outlier_count
            FROM {self.staging_table}
            WHERE current_balance < %s OR current_balance > %s
            """
            outlier_result = self.db_manager.model_db.execute_query(
                outlier_query, (lower_bound, upper_bound)
            )
            if outlier_result:
                logger.info(f"  - Outliers detected: {outlier_result[0]['outlier_count']}")


def main():
    """Main function for command-line execution."""
    parser = argparse.ArgumentParser(description='Create staging account daily snapshots')
    parser.add_argument('--start-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='Start date for snapshot creation (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=lambda s: datetime.strptime(s, '%Y-%m-%d').date(),
                       help='End date for snapshot creation (YYYY-MM-DD)')
    parser.add_argument('--force-rebuild', action='store_true',
                       help='Force rebuild of existing snapshots')
    parser.add_argument('--clean-data', action='store_true',
                       help='Run data cleaning after creating snapshots')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='Set logging level')
    
    args = parser.parse_args()
    
    # Set up logging
    setup_logging(log_level=args.log_level, log_file='create_staging_snapshots')
    
    # Run snapshot creation
    creator = StagingSnapshotCreator()
    try:
        records = creator.create_snapshots(
            start_date=args.start_date,
            end_date=args.end_date,
            force_rebuild=args.force_rebuild
        )
        logger.info(f"Snapshot creation complete. Total records: {records}")
        
        # Run data cleaning if requested
        if args.clean_data:
            creator.clean_data()
        
    except Exception as e:
        logger.error(f"Snapshot creation failed: {str(e)}")
        raise


if __name__ == '__main__':
    main()

================
File: src/utils/__init__.py
================
# Utility modules for the daily profit model

================
File: src/utils/api_client.py
================
"""
API client utilities for interacting with the risk analytics API endpoints.
Handles authentication, pagination, and rate limiting.
"""

import os
import time
import logging
from typing import Dict, List, Any, Optional, Iterator
from datetime import datetime, timedelta
import requests
from urllib.parse import urljoin, urlencode
import json

logger = logging.getLogger(__name__)


class RiskAnalyticsAPIClient:
    """Client for interacting with the Risk Analytics TFT External API."""
    
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        Initialize API client.
        
        Args:
            api_key: API key for authentication (defaults to env var API_KEY)
            base_url: Base URL for the API (defaults to env var API_BASE_URL)
        """
        self.api_key = api_key or os.getenv('API_KEY')
        if not self.api_key:
            raise ValueError("API key is required. Set API_KEY environment variable.")
        
        self.base_url = base_url or os.getenv(
            'API_BASE_URL', 
            'https://easton.apis.arizet.io/risk-analytics/tft/external/'
        )
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'DailyProfitModel/1.0',
            'Accept': 'application/json'
        })
        
        # Rate limiting configuration
        self.requests_per_second = 10  # Adjust based on API limits
        self.last_request_time = 0
        
        logger.info(f"API client initialized for {self.base_url}")
    
    def _rate_limit(self):
        """Implement rate limiting to avoid overwhelming the API."""
        min_interval = 1.0 / self.requests_per_second
        elapsed = time.time() - self.last_request_time
        if elapsed < min_interval:
            time.sleep(min_interval - elapsed)
        self.last_request_time = time.time()
    
    def _make_request(self, 
                     endpoint: str, 
                     method: str = 'GET',
                     params: Optional[Dict[str, Any]] = None,
                     data: Optional[Dict[str, Any]] = None,
                     timeout: int = 30) -> Dict[str, Any]:
        """
        Make an API request with error handling and retries.
        
        Args:
            endpoint: API endpoint path
            method: HTTP method (GET, POST, etc.)
            params: Query parameters
            data: Request body data
            timeout: Request timeout in seconds
            
        Returns:
            Response data as dictionary
        """
        url = urljoin(self.base_url, endpoint)
        
        # Add API key to params
        if params is None:
            params = {}
        params['apiKey'] = self.api_key
        
        # Rate limiting
        self._rate_limit()
        
        # Retry configuration
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = self.session.request(
                    method=method,
                    url=url,
                    params=params,
                    json=data,
                    timeout=timeout
                )
                
                # Log request details
                logger.debug(f"{method} {url} - Status: {response.status_code}")
                
                # Check for successful response
                response.raise_for_status()
                
                # Parse JSON response
                return response.json()
                
            except requests.exceptions.Timeout:
                logger.warning(f"Request timeout on attempt {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise
                    
            except requests.exceptions.HTTPError as e:
                if response.status_code == 429:  # Rate limit exceeded
                    logger.warning("Rate limit exceeded, backing off...")
                    time.sleep(retry_delay * 5)
                    retry_delay *= 2
                elif response.status_code >= 500:  # Server error
                    logger.warning(f"Server error {response.status_code} on attempt {attempt + 1}")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        retry_delay *= 2
                    else:
                        raise
                else:
                    # Client error, don't retry
                    logger.error(f"Client error {response.status_code}: {response.text}")
                    raise
                    
            except Exception as e:
                logger.error(f"Unexpected error: {str(e)}")
                raise
    
    def paginate(self,
                endpoint: str,
                params: Optional[Dict[str, Any]] = None,
                limit: int = 1000,
                max_pages: Optional[int] = None) -> Iterator[List[Dict[str, Any]]]:
        """
        Paginate through API results.
        
        Args:
            endpoint: API endpoint path
            params: Query parameters
            limit: Number of results per page
            max_pages: Maximum number of pages to fetch (optional)
            
        Yields:
            Lists of results from each page
        """
        if params is None:
            params = {}
        
        params['limit'] = limit
        params['skip'] = 0
        page = 0
        
        while max_pages is None or page < max_pages:
            logger.info(f"Fetching page {page + 1}, skip={params['skip']}, limit={limit}")
            
            response = self._make_request(endpoint, params=params)
            
            # Handle different response formats
            if isinstance(response, list):
                results = response
            elif isinstance(response, dict) and 'data' in response:
                results = response['data']
            else:
                logger.warning(f"Unexpected response format: {type(response)}")
                results = []
            
            if not results:
                logger.info("No more results, stopping pagination")
                break
            
            yield results
            
            # Check if we got fewer results than limit (last page)
            if len(results) < limit:
                logger.info(f"Last page reached (got {len(results)} results)")
                break
            
            # Update pagination parameters
            params['skip'] += limit
            page += 1
            
            # Small delay between pages
            time.sleep(0.1)
    
    def get_accounts(self, 
                    logins: Optional[List[str]] = None,
                    traders: Optional[List[str]] = None,
                    **kwargs) -> Iterator[List[Dict[str, Any]]]:
        """
        Get accounts data with pagination.
        
        Args:
            logins: List of login IDs to filter
            traders: List of trader IDs to filter
            **kwargs: Additional parameters
            
        Yields:
            Lists of account records
        """
        params = {}
        if logins:
            params['logins'] = ','.join(logins)
        if traders:
            params['traders'] = ','.join(traders)
        params.update(kwargs)
        
        # Max limit for accounts endpoint is 500
        yield from self.paginate('accounts', params=params, limit=500)
    
    def get_metrics(self,
                   metric_type: str,
                   logins: Optional[List[str]] = None,
                   accountids: Optional[List[str]] = None,
                   dates: Optional[List[str]] = None,
                   hours: Optional[List[int]] = None,
                   **kwargs) -> Iterator[List[Dict[str, Any]]]:
        """
        Get metrics data with pagination.
        
        Args:
            metric_type: Type of metrics ('alltime', 'daily', 'hourly')
            logins: List of login IDs to filter
            accountids: List of account IDs to filter
            dates: List of dates in YYYYMMDD format
            hours: List of hours (0-23) for hourly metrics
            **kwargs: Additional parameters
            
        Yields:
            Lists of metrics records
        """
        endpoint = f'v2/metrics/{metric_type}'
        params = {}
        
        if logins:
            params['logins'] = ','.join(logins)
        if accountids:
            params['accountids'] = ','.join(accountids)
        if dates:
            params['dates'] = ','.join(dates)
        if hours and metric_type == 'hourly':
            params['hours'] = ','.join(map(str, hours))
        params.update(kwargs)
        
        # Max limit for metrics endpoints is 1000
        yield from self.paginate(endpoint, params=params, limit=1000)
    
    def get_trades(self,
                  trade_type: str,
                  logins: Optional[List[str]] = None,
                  symbols: Optional[List[str]] = None,
                  open_time_from: Optional[str] = None,
                  open_time_to: Optional[str] = None,
                  close_time_from: Optional[str] = None,
                  close_time_to: Optional[str] = None,
                  trade_date_from: Optional[str] = None,
                  trade_date_to: Optional[str] = None,
                  **kwargs) -> Iterator[List[Dict[str, Any]]]:
        """
        Get trades data with pagination.
        
        Args:
            trade_type: Type of trades ('closed' or 'open')
            logins: List of login IDs to filter
            symbols: List of symbols to filter
            open_time_from/to: Open time range in YYYYMMDD format
            close_time_from/to: Close time range in YYYYMMDD format (closed trades only)
            trade_date_from/to: Trade date range in YYYYMMDD format
            **kwargs: Additional parameters
            
        Yields:
            Lists of trade records
        """
        endpoint = f'v2/trades/{trade_type}'
        params = {}
        
        if logins:
            params['logins'] = ','.join(logins)
        if symbols:
            params['symbols'] = ','.join(symbols)
        
        # Date filters
        if open_time_from:
            params['open-time-from'] = open_time_from
        if open_time_to:
            params['open-time-to'] = open_time_to
        if close_time_from and trade_type == 'closed':
            params['close-time-from'] = close_time_from
        if close_time_to and trade_type == 'closed':
            params['close-time-to'] = close_time_to
        if trade_date_from:
            params['trade-date-from'] = trade_date_from
        if trade_date_to:
            params['trade-date-to'] = trade_date_to
        
        params.update(kwargs)
        
        # Max limit for trades endpoints is 1000
        yield from self.paginate(endpoint, params=params, limit=1000)
    
    def format_date(self, date: datetime) -> str:
        """Format date for API in YYYYMMDD format."""
        return date.strftime('%Y%m%d')
    
    def close(self):
        """Close the session."""
        self.session.close()
        logger.info("API client session closed")

================
File: src/utils/database.py
================
"""
Database connection utilities for the daily profit model.
Handles connections to both the source Supabase database and the model's PostgreSQL schema.
"""

import os
import logging
from typing import Optional, Dict, Any, List
from contextlib import contextmanager
import psycopg2
from psycopg2.extras import RealDictCursor, execute_batch
from psycopg2.pool import SimpleConnectionPool
import pandas as pd
from datetime import datetime

logger = logging.getLogger(__name__)


class DatabaseConnection:
    """Manages PostgreSQL database connections with connection pooling."""
    
    def __init__(self, 
                 host: str,
                 port: int,
                 database: str,
                 user: str,
                 password: str,
                 schema: Optional[str] = None,
                 min_connections: int = 1,
                 max_connections: int = 10):
        """
        Initialize database connection manager.
        
        Args:
            host: Database host
            port: Database port
            database: Database name
            user: Database user
            password: Database password
            schema: Default schema to use (optional)
            min_connections: Minimum number of connections in pool
            max_connections: Maximum number of connections in pool
        """
        self.connection_params = {
            'host': host,
            'port': port,
            'database': database,
            'user': user,
            'password': password
        }
        self.schema = schema
        self.pool = None
        self._initialize_pool(min_connections, max_connections)
    
    def _initialize_pool(self, min_connections: int, max_connections: int):
        """Initialize connection pool."""
        try:
            self.pool = SimpleConnectionPool(
                min_connections,
                max_connections,
                **self.connection_params
            )
            logger.info(f"Database connection pool initialized for {self.connection_params['host']}")
        except Exception as e:
            logger.error(f"Failed to initialize connection pool: {str(e)}")
            raise
    
    @contextmanager
    def get_connection(self):
        """
        Context manager for database connections.
        Automatically handles connection checkout/return from pool.
        """
        connection = None
        try:
            connection = self.pool.getconn()
            if self.schema:
                with connection.cursor() as cursor:
                    cursor.execute(f"SET search_path TO {self.schema}")
            yield connection
            connection.commit()
        except Exception as e:
            if connection:
                connection.rollback()
            logger.error(f"Database error: {str(e)}")
            raise
        finally:
            if connection:
                self.pool.putconn(connection)
    
    def execute_query(self, query: str, params: Optional[tuple] = None) -> List[Dict[str, Any]]:
        """
        Execute a SELECT query and return results as list of dictionaries.
        
        Args:
            query: SQL query to execute
            params: Query parameters (optional)
            
        Returns:
            List of dictionaries representing query results
        """
        with self.get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute(query, params)
                return cursor.fetchall()
    
    def execute_query_df(self, query: str, params: Optional[tuple] = None) -> pd.DataFrame:
        """
        Execute a SELECT query and return results as pandas DataFrame.
        
        Args:
            query: SQL query to execute
            params: Query parameters (optional)
            
        Returns:
            pandas DataFrame with query results
        """
        with self.get_connection() as conn:
            return pd.read_sql_query(query, conn, params=params)
    
    def execute_command(self, command: str, params: Optional[tuple] = None) -> int:
        """
        Execute an INSERT/UPDATE/DELETE command.
        
        Args:
            command: SQL command to execute
            params: Command parameters (optional)
            
        Returns:
            Number of affected rows
        """
        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(command, params)
                return cursor.rowcount
    
    def insert_batch(self, table: str, data: List[Dict[str, Any]], 
                    page_size: int = 1000, returning: Optional[str] = None) -> List[Any]:
        """
        Insert multiple rows efficiently using execute_batch.
        
        Args:
            table: Table name
            data: List of dictionaries containing row data
            page_size: Batch size for inserts
            returning: Column to return after insert (optional)
            
        Returns:
            List of returned values if returning is specified
        """
        if not data:
            return []
        
        # Get column names from first row
        columns = list(data[0].keys())
        placeholders = ', '.join(['%s'] * len(columns))
        columns_str = ', '.join(columns)
        
        query = f"INSERT INTO {table} ({columns_str}) VALUES ({placeholders})"
        if returning:
            query += f" RETURNING {returning}"
        
        values = [[row.get(col) for col in columns] for row in data]
        
        returned_values = []
        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                if returning:
                    for batch in values:
                        cursor.execute(query, batch)
                        returned_values.extend([row[0] for row in cursor.fetchall()])
                else:
                    execute_batch(cursor, query, values, page_size=page_size)
        
        logger.info(f"Inserted {len(data)} rows into {table}")
        return returned_values
    
    def table_exists(self, table_name: str) -> bool:
        """Check if a table exists in the database."""
        query = """
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = COALESCE(%s, 'public')
            AND table_name = %s
        )
        """
        result = self.execute_query(query, (self.schema, table_name))
        return result[0]['exists'] if result else False
    
    def get_table_row_count(self, table_name: str) -> int:
        """Get the number of rows in a table."""
        query = f"SELECT COUNT(*) as count FROM {table_name}"
        result = self.execute_query(query)
        return result[0]['count'] if result else 0
    
    def close(self):
        """Close all connections in the pool."""
        if self.pool:
            self.pool.closeall()
            logger.info("Database connection pool closed")


class DatabaseManager:
    """Manages connections to both source and model databases."""
    
    def __init__(self):
        """Initialize database manager with connections from environment variables."""
        # Model database connection (prop_trading_model schema)
        self.model_db = DatabaseConnection(
            host=os.getenv('DB_HOST', 'localhost'),
            port=int(os.getenv('DB_PORT', '5432')),
            database=os.getenv('DB_NAME', 'postgres'),
            user=os.getenv('DB_USER', 'postgres'),
            password=os.getenv('DB_PASSWORD', ''),
            schema='prop_trading_model'
        )
        
        # Source database connection (for regimes_daily)
        self.source_db = DatabaseConnection(
            host=os.getenv('DB_HOST', 'localhost'),
            port=int(os.getenv('DB_PORT', '5432')),
            database=os.getenv('DB_NAME', 'postgres'),
            user=os.getenv('DB_USER', 'postgres'),
            password=os.getenv('DB_PASSWORD', ''),
            schema='public'  # regimes_daily is in public schema
        )
        
        logger.info("Database manager initialized")
    
    def log_pipeline_execution(self, 
                             pipeline_stage: str,
                             execution_date: datetime,
                             status: str,
                             records_processed: Optional[int] = None,
                             error_message: Optional[str] = None,
                             execution_details: Optional[Dict[str, Any]] = None):
        """
        Log pipeline execution details to the database.
        
        Args:
            pipeline_stage: Name of the pipeline stage
            execution_date: Date of execution
            status: Execution status ('running', 'success', 'failed')
            records_processed: Number of records processed (optional)
            error_message: Error message if failed (optional)
            execution_details: Additional execution details as JSON (optional)
        """
        import json
        
        query = """
        INSERT INTO pipeline_execution_log 
        (pipeline_stage, execution_date, start_time, end_time, status, 
         records_processed, error_message, execution_details)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (pipeline_stage, execution_date) DO UPDATE
        SET end_time = EXCLUDED.end_time,
            status = EXCLUDED.status,
            records_processed = EXCLUDED.records_processed,
            error_message = EXCLUDED.error_message,
            execution_details = EXCLUDED.execution_details
        """
        
        now = datetime.now()
        params = (
            pipeline_stage,
            execution_date,
            now if status == 'running' else None,
            now if status in ['success', 'failed'] else None,
            status,
            records_processed,
            error_message,
            json.dumps(execution_details) if execution_details else None
        )
        
        try:
            self.model_db.execute_command(query, params)
            logger.info(f"Logged pipeline execution: {pipeline_stage} - {status}")
        except Exception as e:
            logger.error(f"Failed to log pipeline execution: {str(e)}")
    
    def close(self):
        """Close all database connections."""
        self.model_db.close()
        self.source_db.close()


# Singleton instance
_db_manager = None


def get_db_manager() -> DatabaseManager:
    """Get or create the database manager singleton."""
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager


def close_db_connections():
    """Close all database connections."""
    global _db_manager
    if _db_manager:
        _db_manager.close()
        _db_manager = None

================
File: src/utils/logging_config.py
================
"""
Logging configuration for the daily profit model.
Sets up consistent logging across all modules.
"""

import logging
import os
from datetime import datetime
from pathlib import Path


def setup_logging(
    log_level: str = None,
    log_file: str = None,
    log_dir: str = None
):
    """
    Set up logging configuration for the application.
    
    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Name of the log file (optional)
        log_dir: Directory for log files (optional)
    """
    # Get log level from environment or use default
    if log_level is None:
        log_level = os.getenv('LOG_LEVEL', 'INFO')
    
    # Create logs directory if specified
    if log_dir is None:
        log_dir = os.getenv('LOG_DIR', 'logs')
    
    log_path = Path(log_dir)
    log_path.mkdir(exist_ok=True)
    
    # Set up log format
    log_format = (
        '%(asctime)s - %(name)s - %(levelname)s - '
        '%(filename)s:%(lineno)d - %(message)s'
    )
    
    # Configure root logger
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format=log_format,
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Add file handler if log file specified
    if log_file:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_path = log_path / f"{log_file}_{timestamp}.log"
        
        file_handler = logging.FileHandler(file_path)
        file_handler.setLevel(getattr(logging, log_level.upper()))
        file_handler.setFormatter(logging.Formatter(log_format))
        
        # Add handler to root logger
        logging.getLogger().addHandler(file_handler)
        
        # Log the setup
        logging.info(f"Logging initialized - Level: {log_level}, File: {file_path}")
    else:
        logging.info(f"Logging initialized - Level: {log_level}")
    
    # Set specific loggers to WARNING to reduce noise
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('requests').setLevel(logging.WARNING)
    
    return logging.getLogger()

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the enitre vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# AI Docs
# folder which contains documentation and implemenation plans to assist AI coding assistants with implementation
ai-docs/

================
File: .python-version
================
3.13

================
File: pyproject.toml
================
[project]
name = "daily-profit-model"
version = "0.1.0"
description = "A predictive model to score trading accounts based on the likelihood and magnitude of making profits, aiding in hedging decisions."
readme = "README.md"
requires-python = ">=3.13"
authors = [
    { name = "Carlos Rico-Ospina", email = "carlosricojr@gmail.com" }
]
keywords = ["finance", "prop trading", "machine learning", "hedging", "prediction"]

dependencies = [
    "lightgbm>=4.6.0",
    "numpy>=2.2.6",
    "optuna>=4.3.0",
    "pandas>=2.2.3",
    "psycopg2-binary>=2.9.10",
    "python-dotenv>=1.1.0",
    "requests>=2.32.3",
    "scikit-learn>=1.6.1",
    "shap>=0.47.2",
    "tqdm>=4.67.1",
]

[project.urls]
"Homepage" = "localhost:69420" # Update if you have one
"Repository" = "https://github.com/carlosricojr/daily-profit-model"

[build-system]
requires = ["uv>=0.2.0"]
build-backend = "uv"

================
File: README.md
================
# Daily Profit Model

A comprehensive machine learning pipeline for predicting daily profit/loss (PnL) for proprietary trading accounts. This model uses LightGBM to predict next-day PnL based on historical performance, behavioral patterns, and market regime features.

## Overview

This project implements a complete end-to-end machine learning pipeline that:

1. **Ingests data** from multiple API endpoints and CSV files
2. **Preprocesses and cleans** the data into a staging layer
3. **Engineers features** from account metrics, trading behavior, and market regimes
4. **Trains a LightGBM model** with time-series aware splitting and hyperparameter tuning
5. **Generates daily predictions** for all eligible funded accounts
6. **Evaluates predictions** against actual outcomes

The model is designed to support risk management decisions by identifying accounts likely to experience significant profits or losses.

## Project Structure

```
daily-profit-model/
├── src/
│   ├── db_schema/
│   │   └── schema.sql              # PostgreSQL database schema
│   ├── data_ingestion/
│   │   ├── ingest_accounts.py      # Ingest account data from API
│   │   ├── ingest_metrics.py       # Ingest metrics (alltime/daily/hourly)
│   │   ├── ingest_trades.py        # Ingest trades (open/closed)
│   │   ├── ingest_plans.py         # Ingest plan data from CSV
│   │   └── ingest_regimes.py       # Ingest market regime data
│   ├── preprocessing/
│   │   └── create_staging_snapshots.py  # Create daily account snapshots
│   ├── feature_engineering/
│   │   ├── engineer_features.py    # Calculate all features
│   │   └── build_training_data.py  # Align features with targets
│   ├── modeling/
│   │   ├── train_model.py          # Train LightGBM model
│   │   └── predict_daily.py        # Generate daily predictions
│   ├── pipeline_orchestration/
│   │   └── run_pipeline.py         # Main orchestration script
│   └── utils/
│       ├── database.py             # Database connection utilities
│       ├── api_client.py           # API client with pagination
│       └── logging_config.py       # Logging configuration
├── model_artifacts/                # Trained models and artifacts
├── logs/                          # Application logs
├── raw-data/
│   └── plans/                     # CSV files with plan data
├── ai-docs/                       # AI-focused documentation
├── pyproject.toml                 # Project dependencies
├── uv.lock                        # Locked dependencies
├── .env                           # Environment variables (not in git)
├── .gitignore
└── README.md                      # This file
```

## Prerequisites

- **Python 3.13.2** (as specified in the roadmap)
- **PostgreSQL** database (Supabase instance)
- **uv** package manager for dependency management
- API key for Risk Analytics API
- Database credentials

## Environment Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/carlosricojr/daily-profit-model.git
   cd daily-profit-model
   ```

2. **Install uv** (if not already installed):
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

3. **Create virtual environment and install dependencies**:
   ```bash
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   uv pip install -r pyproject.toml
   ```

4. **Set up environment variables**:
   Create a `.env` file in the project root with:
   ```env
   # API Configuration
   API_KEY=your_api_key_here
   API_BASE_URL=https://easton.apis.arizet.io/risk-analytics/tft/external/
   
   # Database Configuration
   DB_HOST=db.yvwwaxmwbkkyepreillh.supabase.co
   DB_PORT=5432
   DB_NAME=postgres
   DB_USER=postgres
   DB_PASSWORD=your_password_here
   
   # Logging
   LOG_LEVEL=INFO
   LOG_DIR=logs
   ```

5. **Create the database schema**:
   ```bash
   uv run --env-file .env -- psql -h $DB_HOST -U $DB_USER -d $DB_NAME -f src/db_schema/schema.sql
   ```

   Or using the pipeline:
   ```bash
   uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --stages schema
   ```

## Running the Pipeline

### Full Pipeline Execution

To run the complete pipeline from data ingestion to predictions:

```bash
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py
```

### Running Specific Stages

You can run individual stages or combinations:

```bash
# Only data ingestion
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --stages ingestion

# Preprocessing and feature engineering
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --stages preprocessing feature_engineering

# Only daily predictions
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --stages prediction
```

### Individual Script Execution

Each component can also be run independently:

#### Data Ingestion
```bash
# Ingest accounts
uv run --env-file .env -- python -m src.data_ingestion.ingest_accounts

# Ingest daily metrics
uv run --env-file .env -- python -m src.data_ingestion.ingest_metrics daily --start-date 2024-01-01

# Ingest closed trades (with batching for 81M records)
uv run --env-file .env -- python -m src.data_ingestion.ingest_trades closed --batch-days 7

# Ingest plans from CSV
uv run --env-file .env -- python -m src.data_ingestion.ingest_plans

# Ingest market regimes
uv run --env-file .env -- python -m src.data_ingestion.ingest_regimes --start-date 2024-01-01
```

#### Preprocessing
```bash
# Create staging snapshots
uv run --env-file .env -- python -m src.preprocessing.create_staging_snapshots --start-date 2024-01-01 --clean-data
```

#### Feature Engineering
```bash
# Engineer features
uv run --env-file .env -- python -m src.feature_engineering.engineer_features --start-date 2024-01-01

# Build training data (aligns features with targets)
uv run --env-file .env -- python -m src.feature_engineering.build_training_data --validate
```

#### Model Training
```bash
# Train model with hyperparameter tuning
uv run --env-file .env -- python -m src.modeling.train_model --tune-hyperparameters --n-trials 50
```

#### Daily Predictions
```bash
# Generate predictions for today
uv run --env-file .env -- python -m src.modeling.predict_daily

# Generate predictions for specific date
uv run --env-file .env -- python -m src.modeling.predict_daily --prediction-date 2024-12-01

# Evaluate yesterday's predictions
uv run --env-file .env -- python -m src.modeling.predict_daily --evaluate
```

## Pipeline Options

The main orchestration script supports several options:

```bash
# Dry run - see what would be executed without running
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --dry-run

# Force re-run of completed stages
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --force

# Specify date range
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py \
    --start-date 2024-01-01 \
    --end-date 2024-12-31

# Set logging level
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --log-level DEBUG
```

## Daily Operation Workflow

For daily operations, you would typically:

1. **Morning: Ingest yesterday's data and generate predictions**
   ```bash
   uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py \
       --stages ingestion preprocessing feature_engineering prediction
   ```

2. **Evening: Evaluate predictions against actuals**
   ```bash
   uv run --env-file .env -- python -m src.modeling.predict_daily --evaluate
   ```

3. **Weekly/Monthly: Retrain model with new data**
   ```bash
   uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py \
       --stages feature_engineering training
   ```

## Model Details

### Features

The model uses several categories of features:

1. **Static Account Features**: Plan details, risk parameters
2. **Dynamic Account State**: Current balance, equity, distances to targets
3. **Historical Performance**: Rolling PnL statistics, win rates, Sharpe ratios
4. **Behavioral Features**: Trading patterns, instrument concentration, risk management
5. **Market Regime Features**: Sentiment scores, volatility regimes, economic indicators
6. **Temporal Features**: Day of week, month, quarter effects

### Target Variable

The target variable is `net_profit` from the daily metrics, representing the PnL for day T+1.

### Model Configuration

- **Algorithm**: LightGBM (gradient boosting)
- **Objective**: MAE, Huber, or Fair (robust to outliers)
- **Hyperparameter Tuning**: Optuna with time-series cross-validation
- **Feature Scaling**: StandardScaler for numerical features
- **Categorical Handling**: Native LightGBM categorical feature support

## Database Schema

The pipeline uses a dedicated `prop_trading_model` schema with tables for:

- Raw data ingestion (`raw_*` tables)
- Staging/preprocessing (`stg_*` tables)
- Feature storage (`feature_store_account_daily`)
- Model training (`model_training_input`)
- Predictions (`model_predictions`)
- Model registry and pipeline logs

## Monitoring and Logging

- **Logs**: Stored in the `logs/` directory with timestamps
- **Pipeline Execution**: Tracked in `pipeline_execution_log` table
- **Model Performance**: Evaluated daily and stored in the database
- **Feature Importance**: Calculated and stored with each model version

## Troubleshooting

### Common Issues

1. **Import errors**: Ensure you're running scripts with `uv run --env-file .env`
2. **Database connection**: Check credentials in `.env` file
3. **API rate limits**: The client implements rate limiting, but adjust if needed
4. **Memory issues**: For large datasets, consider increasing batch sizes in ingestion scripts
5. **Missing data**: Check pipeline logs for specific dates/accounts

### Debugging

Enable debug logging:
```bash
uv run --env-file .env -- python src/pipeline_orchestration/run_pipeline.py --log-level DEBUG
```

Check pipeline status:
```sql
SELECT * FROM prop_trading_model.pipeline_execution_log 
ORDER BY created_at DESC LIMIT 10;
```

## Contributing

1. Follow the existing code structure and naming conventions
2. Add appropriate logging and error handling
3. Update documentation for new features
4. Test thoroughly with time-series data

## License

[Specify your license here]

## Contact

For questions or issues, please contact the development team or create an issue in the repository.



================================================================
End of Codebase
================================================================
